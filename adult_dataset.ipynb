{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d410884",
   "metadata": {},
   "source": [
    "# Dataset Context and Exploration\n",
    "\n",
    "**age**  \n",
    "- Demographic: Age  \n",
    "- Description: How old is this sample of people  \n",
    "- Data Type: Integer  \n",
    "- Missing Values: No  \n",
    "\n",
    "**workclass**  \n",
    "- Demographic: Income  \n",
    "- Description: Employment classification  \n",
    "- Possible values: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked  \n",
    "- Data Type: Categorical  \n",
    "- Missing Values: Yes  \n",
    "\n",
    "**fnlwgt**  \n",
    "- Demographic:  \n",
    "- Description: Final weight representing the number of people the census believes the entry represents  \n",
    "- Data Type: Integer  \n",
    "- Missing Values: No  \n",
    "\n",
    "**education**  \n",
    "- Demographic: Education Level  \n",
    "- Description: Highest level of education achieved  \n",
    "- Possible values: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool  \n",
    "- Data Type: Categorical  \n",
    "- Missing Values: No  \n",
    "\n",
    "**education-num**  \n",
    "- Demographic: Education Level  \n",
    "- Description: Numeric representation of education level  \n",
    "- Data Type: Integer  \n",
    "- Missing Values: No  \n",
    "\n",
    "**marital-status**  \n",
    "- Demographic: Other  \n",
    "- Description: Marital status of the individual  \n",
    "- Possible values: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse  \n",
    "- Data Type: Categorical  \n",
    "- Missing Values: No  \n",
    "\n",
    "**occupation**  \n",
    "- Demographic: Other  \n",
    "- Description: Type of job  \n",
    "- Possible values: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces  \n",
    "- Data Type: Categorical  \n",
    "- Missing Values: Yes  \n",
    "\n",
    "**relationship**  \n",
    "- Demographic: Other  \n",
    "- Description: Relationship within household  \n",
    "- Possible values: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried  \n",
    "- Data Type: Categorical  \n",
    "- Missing Values: No  \n",
    "\n",
    "**race**  \n",
    "- Demographic: Race  \n",
    "- Description: Race of the individual  \n",
    "- Possible values: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black  \n",
    "- Data Type: Categorical  \n",
    "- Missing Values: No  \n",
    "\n",
    "**sex**  \n",
    "- Demographic: Sex  \n",
    "- Description: Gender of the individual  \n",
    "- Possible values: Female, Male  \n",
    "- Data Type: Binary  \n",
    "- Missing Values: No  \n",
    "\n",
    "**capital-gain**  \n",
    "- Demographic:  \n",
    "- Description: Income from capital gains  \n",
    "- Data Type: Integer  \n",
    "- Missing Values: No  \n",
    "\n",
    "**capital-loss**  \n",
    "- Demographic:  \n",
    "- Description: Capital losses  \n",
    "- Data Type: Integer  \n",
    "- Missing Values: No  \n",
    "\n",
    "**hours-per-week**  \n",
    "- Demographic:  \n",
    "- Description: Number of hours worked per week  \n",
    "- Data Type: Integer  \n",
    "- Missing Values: No  \n",
    "\n",
    "**native-country**  \n",
    "- Demographic: Other  \n",
    "- Description: Country of origin  \n",
    "- Possible values: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands  \n",
    "- Data Type: Categorical  \n",
    "- Missing Values: Yes  \n",
    "\n",
    "**income**  \n",
    "- Demographic: Income  \n",
    "- Description: Income class  \n",
    "- Possible values: >50K, <=50K  \n",
    "- Data Type: Binary  \n",
    "- Missing Values: No  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cfb6049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe9ed00",
   "metadata": {},
   "source": [
    "# Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1f607c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                  0\n",
      "workclass         2799\n",
      "fnlwgt               0\n",
      "education            0\n",
      "education-num        0\n",
      "marital-status       0\n",
      "occupation        2809\n",
      "relationship         0\n",
      "race                 0\n",
      "sex                  0\n",
      "capital-gain         0\n",
      "capital-loss         0\n",
      "hours-per-week       0\n",
      "native-country     857\n",
      "income               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('base_data/adultdataset.csv', na_values='NaN', skipinitialspace=True)\n",
    "\n",
    "#Identify and count the number of missing values in each column, the missing values are repesented by \"NaN\"\n",
    "missing_values = df.isna().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca1cdf5",
   "metadata": {},
   "source": [
    "# Total fnlwgt\n",
    "\n",
    "The fnlwgt represent the weight a certain row has over the whole dataset, this works like a percentage.\n",
    "From this point forward if we discribe something as weighted it means that it weighted to the total fnlwgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3a987500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9263575662\n"
     ]
    }
   ],
   "source": [
    "total_fnlwgt = df['fnlwgt'].sum()\n",
    "print(total_fnlwgt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57702e4b",
   "metadata": {},
   "source": [
    "# Weighted Age Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "376abf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'17-26': 0.23254280804728858, '27-36': 0.27141756733459493, '37-46': 0.23722836582580933, '47-56': 0.15117445207951713, '57-66': 0.07870687525021912, '67-76': 0.02373170685071477, '77-86': 0.004034587978086513, '87-99': 0.001163636633769635}\n"
     ]
    }
   ],
   "source": [
    "age_bins = [(17, 26), (27, 36), (37, 46), (47, 56), (57, 66), (67, 76), (77, 86), (87, 99)]\n",
    "\n",
    "age_distribution = (df.groupby('age')['fnlwgt'].sum() / total_fnlwgt).to_dict()\n",
    "\n",
    "# Aggregate into custom age ranges\n",
    "age_ranges = {}\n",
    "for start, end in age_bins:\n",
    "    label = f'{start}-{end}'\n",
    "    age_ranges[label] = sum(age_distribution.get(age, 0) for age in range(start, end + 1))\n",
    "\n",
    "print(age_ranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cf6cee",
   "metadata": {},
   "source": [
    "# Weighted Education Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5bfcbf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10th': 0.029468504815025498, '11th': 0.03816294386736559, '12th': 0.01400998833877717, '1st-4th': 0.0062664863027056044, '5th-6th': 0.012629909796056028, '7th-8th': 0.019338862285637366, '9th': 0.016240940376528228, 'Assoc-acdm': 0.03347668679083759, 'Assoc-voc': 0.0399183750953639, 'Bachelors': 0.163176883759985, 'Doctorate': 0.011804269538010278, 'HS-grad': 0.32139634430936437, 'Masters': 0.0520525309657691, 'Preschool': 0.0021408804465594703, 'Prof-school': 0.016798332164364632, 'Some-college': 0.22311806114765018}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "education_distribution = (df.groupby('education')['fnlwgt'].sum() / total_fnlwgt).to_dict()\n",
    "print(education_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7d20fb",
   "metadata": {},
   "source": [
    "# Weighted Race Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bed862e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Amer-Indian-Eskimo': 0.00609399275827926, 'Asian-Pac-Islander': 0.026202793700461278, 'Black': 0.11656649596219383, 'Other': 0.008577742860778288, 'White': 0.8425589747182873}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "race_distribution = (df.groupby('race')['fnlwgt'].sum() / total_fnlwgt).to_dict()\n",
    "print(race_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bb02c1",
   "metadata": {},
   "source": [
    "# Weighted Native Country Distribuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e324f64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cambodia': 0.0006054133095717787, 'Canada': 0.0035612337183499113, 'China': 0.002275493585750992, 'Columbia': 0.001998964619672796, 'Cuba': 0.003584282917470276, 'Dominican-Republic': 0.002264667852399304, 'Ecuador': 0.000867478746135166, 'El-Salvador': 0.004194289701695103, 'England': 0.0025167153430435273, 'France': 0.0007650541495625774, 'Germany': 0.004291807337752816, 'Greece': 0.000795958306925307, 'Guatemala': 0.0024352646130521775, 'Haiti': 0.0017626972128033124, 'Holand-Netherlands': 3.009852892374422e-06, 'Honduras': 0.0005169305217253592, 'Hong': 0.0006895137723332388, 'Hungary': 0.00040688543360871404, 'India': 0.0026994449996861265, 'Iran': 0.0012345983254516651, 'Ireland': 0.000583518308397231, 'Italy': 0.0020298072457196714, 'Jamaica': 0.002418630971181892, 'Japan': 0.0019346626674100781, 'Laos': 0.0005085181113512882, 'Mexico': 0.02920982867446892, 'Nicaragua': 0.0015055085108452585, 'Outlying-US(Guam-USVI-etc)': 0.00046019119997985937, 'Peru': 0.0013488914492551592, 'Philippines': 0.005207780964956726, 'Poland': 0.0017265999203321452, 'Portugal': 0.0010919792064197425, 'Puerto-Rico': 0.004064267230465031, 'Scotland': 0.0003546663966352996, 'South': 0.0020754486929779234, 'Taiwan': 0.0012956487254953454, 'Thailand': 0.0005933743298009886, 'Trinadad&Tobago': 0.0006071571286530288, 'United-States': 0.8854902228141374, 'Vietnam': 0.0015862030533496602, 'Yugoslavia': 0.0005276729179264515}\n"
     ]
    }
   ],
   "source": [
    "ncountry_distribution = (df.groupby('native-country')['fnlwgt'].sum() / total_fnlwgt).to_dict()\n",
    "print(ncountry_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6b6cb8",
   "metadata": {},
   "source": [
    "# Weighted Workclass Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5416f357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Federal-gov': 0.028380069488549937, 'Local-gov': 0.06437528452930555, 'Never-worked': 0.00023212775265828018, 'Private': 0.7051966278850047, 'Self-emp-inc': 0.03275067868712141, 'Self-emp-not-inc': 0.07319917748192728, 'State-gov': 0.03890616400732109, 'Without-pay': 0.00038062581109622506}\n"
     ]
    }
   ],
   "source": [
    "workclass_distribution = (df.groupby('workclass')['fnlwgt'].sum() / total_fnlwgt).to_dict()\n",
    "print(workclass_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05717e4",
   "metadata": {},
   "source": [
    "# Weighted Income Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dc469a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<=50K': 0.7622240390354357, '>50K': 0.2377759609645643}\n"
     ]
    }
   ],
   "source": [
    "income_distribution = (df.groupby('income')['fnlwgt'].sum() / total_fnlwgt).to_dict()\n",
    "print(income_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163346c7",
   "metadata": {},
   "source": [
    "# Weighted Sex Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "91d60d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Female': 0.32424719304894256, 'Male': 0.6757528069510574}\n"
     ]
    }
   ],
   "source": [
    "sex_distribution = (df.groupby('sex')['fnlwgt'].sum() / total_fnlwgt).to_dict()\n",
    "print(sex_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be109aee",
   "metadata": {},
   "source": [
    "# Weighted Relationship Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6b0598e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Husband': 0.39841025891757864, 'Not-in-family': 0.25856131437726904, 'Other-relative': 0.03308744508422627, 'Own-child': 0.15858494253209673, 'Unmarried': 0.10587409719372355, 'Wife': 0.04548194189510577}\n"
     ]
    }
   ],
   "source": [
    "relationship_distribution = (df.groupby('relationship')['fnlwgt'].sum() / total_fnlwgt).to_dict()\n",
    "print(relationship_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e18149",
   "metadata": {},
   "source": [
    "# Weighted Average Hours per Weak Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3600a96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.0005355909187801483, 2: 0.001110879143807656, 3: 0.0011775734768106653, 4: 0.001502640719727734, 5: 0.001782997041601753, 6: 0.0017477536310751623, 7: 0.0008876964252294569, 8: 0.004306831773788993, 9: 0.0005425860578457053, 10: 0.00822093884464027, 11: 0.00029105478255659766, 12: 0.00464480234954009, 13: 0.0005934932903449739, 14: 0.001068289757765461, 15: 0.012683411491090814, 16: 0.005952873923857416, 17: 0.0009196336610004795, 18: 0.002499484415610746, 19: 0.00037384246929681955, 20: 0.03775265175785208, 21: 0.0009668757860683624, 22: 0.0012101099412383685, 23: 0.000869856013920686, 24: 0.007465276748769972, 25: 0.020016737139702544, 26: 0.0008324194977574308, 27: 0.0008371259957222337, 28: 0.0029465861775135357, 29: 0.0003484678182358651, 30: 0.035563565627408374, 31: 0.00024778768844258833, 32: 0.008798474473991942, 33: 0.0013127072572940571, 34: 0.0009296414596500113, 35: 0.039750254268501276, 36: 0.007376472162936601, 37: 0.004874414119077986, 38: 0.014070435839875153, 39: 0.0011599365506429218, 40: 0.4750028149551481, 41: 0.0010720054935953304, 42: 0.006574459498380249, 43: 0.004373176349838724, 44: 0.006600522652480711, 45: 0.05462403066190879, 46: 0.002475315778340539, 47: 0.0016496344994175534, 48: 0.0162560565697898, 49: 0.0007953133076056048, 50: 0.08643813266238533, 51: 0.0003386656637208994, 52: 0.0043745867123679135, 53: 0.0007236105413913981, 54: 0.0012943107971993806, 55: 0.020353896257732103, 56: 0.002779384110351319, 57: 0.00026824048193324943, 58: 0.0007148367154989401, 59: 0.00014840543761513242, 60: 0.043008709545530134, 61: 7.752902617788323e-05, 62: 0.0003895966451490943, 63: 0.0003774625617129223, 64: 0.00042595603943586594, 65: 0.007006211463920572, 66: 0.0004102484978440283, 67: 9.925357481254628e-05, 68: 0.00022206727456640274, 69: 2.1828180324523158e-05, 70: 0.008170033015486801, 72: 0.002030608340272225, 73: 6.930606748683779e-05, 74: 5.515926232416409e-05, 75: 0.002009212606353514, 76: 8.792296082182094e-05, 77: 0.00022777176729449377, 78: 0.00029280540246749486, 79: 4.941504411496002e-06, 80: 0.004183963127649575, 81: 6.459751847817315e-05, 82: 3.0540043102419042e-06, 84: 0.0011761069804494678, 85: 0.0002735752470178291, 86: 5.313391048546066e-05, 87: 9.336783457687702e-06, 88: 0.00012357172238531235, 89: 3.8341350355346186e-05, 90: 0.000844305944634708, 91: 4.110939597138037e-05, 92: 7.604730891223761e-05, 94: 3.0337097709776337e-05, 95: 3.228450988173081e-05, 96: 0.0002801515413368683, 97: 3.592813532740593e-05, 98: 0.0002679081048779585, 99: 0.0024500539346919837}\n",
      "{40: 0.4750028149551481, 50: 0.08643813266238533, 45: 0.05462403066190879, 60: 0.043008709545530134, 35: 0.039750254268501276, 20: 0.03775265175785208, 30: 0.035563565627408374, 55: 0.020353896257732103, 25: 0.020016737139702544, 48: 0.0162560565697898, 38: 0.014070435839875153, 15: 0.012683411491090814, 32: 0.008798474473991942, 10: 0.00822093884464027, 70: 0.008170033015486801, 24: 0.007465276748769972, 36: 0.007376472162936601, 65: 0.007006211463920572, 44: 0.006600522652480711, 42: 0.006574459498380249, 16: 0.005952873923857416, 37: 0.004874414119077986, 12: 0.00464480234954009, 52: 0.0043745867123679135, 43: 0.004373176349838724, 8: 0.004306831773788993, 80: 0.004183963127649575, 28: 0.0029465861775135357, 56: 0.002779384110351319, 18: 0.002499484415610746, 46: 0.002475315778340539, 99: 0.0024500539346919837, 72: 0.002030608340272225, 75: 0.002009212606353514, 5: 0.001782997041601753, 6: 0.0017477536310751623, 47: 0.0016496344994175534, 4: 0.001502640719727734, 33: 0.0013127072572940571, 54: 0.0012943107971993806, 22: 0.0012101099412383685, 3: 0.0011775734768106653, 84: 0.0011761069804494678, 39: 0.0011599365506429218, 2: 0.001110879143807656, 41: 0.0010720054935953304, 14: 0.001068289757765461, 21: 0.0009668757860683624, 34: 0.0009296414596500113, 17: 0.0009196336610004795, 7: 0.0008876964252294569, 23: 0.000869856013920686, 90: 0.000844305944634708, 27: 0.0008371259957222337, 26: 0.0008324194977574308, 49: 0.0007953133076056048, 53: 0.0007236105413913981, 58: 0.0007148367154989401, 13: 0.0005934932903449739, 9: 0.0005425860578457053, 1: 0.0005355909187801483, 64: 0.00042595603943586594, 66: 0.0004102484978440283, 62: 0.0003895966451490943, 63: 0.0003774625617129223, 19: 0.00037384246929681955, 29: 0.0003484678182358651, 51: 0.0003386656637208994, 78: 0.00029280540246749486, 11: 0.00029105478255659766, 96: 0.0002801515413368683, 85: 0.0002735752470178291, 57: 0.00026824048193324943, 98: 0.0002679081048779585, 31: 0.00024778768844258833, 77: 0.00022777176729449377, 68: 0.00022206727456640274, 59: 0.00014840543761513242, 88: 0.00012357172238531235, 67: 9.925357481254628e-05, 76: 8.792296082182094e-05, 61: 7.752902617788323e-05, 92: 7.604730891223761e-05, 73: 6.930606748683779e-05, 81: 6.459751847817315e-05, 74: 5.515926232416409e-05, 86: 5.313391048546066e-05, 91: 4.110939597138037e-05, 89: 3.8341350355346186e-05, 97: 3.592813532740593e-05, 95: 3.228450988173081e-05, 94: 3.0337097709776337e-05, 69: 2.1828180324523158e-05, 87: 9.336783457687702e-06, 79: 4.941504411496002e-06, 82: 3.0540043102419042e-06}\n",
      "{'<40': 0.2381721704989729, '40': 0.4750028149551481, '>40': 0.28682501454587894}\n",
      "Average Hours: 40.32911200806684\n"
     ]
    }
   ],
   "source": [
    "hours_per_week_distribution = (df.groupby('hours-per-week')['fnlwgt'].sum() / total_fnlwgt).to_dict()\n",
    "print(hours_per_week_distribution)\n",
    "sorted_hours_per_week = dict(sorted(hours_per_week_distribution.items(), key=lambda item: item[1], reverse=True))\n",
    "print(sorted_hours_per_week)\n",
    "\n",
    "hours_per_week_distribution_grouped = {\"<40\":0, \"40\":0, \">40\":0}\n",
    "for key, value in hours_per_week_distribution.items():\n",
    "    if key < 40:\n",
    "        hours_per_week_distribution_grouped[\"<40\"] += value\n",
    "    elif key == 40:\n",
    "        hours_per_week_distribution_grouped[\"40\"] += value\n",
    "    else:\n",
    "        hours_per_week_distribution_grouped[\">40\"] += value\n",
    "print(hours_per_week_distribution_grouped)\n",
    "\n",
    "\n",
    "average_hours_per_week = 0\n",
    "for key, value in hours_per_week_distribution.items():\n",
    "    average_hours_per_week += key * value\n",
    "print(\"Average Hours:\" , average_hours_per_week)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd6877f",
   "metadata": {},
   "source": [
    "# Class Balance of the Dataset\n",
    "\n",
    "Check the distribution of the target variable (the class you’re trying to predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7d395c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "income\n",
       "<=50K    0.760718\n",
       ">50K     0.239282\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['income'].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca8522a",
   "metadata": {},
   "source": [
    "## **Preprocessing the Dataset**\n",
    "\n",
    "In this section, we handle the preprocessing of the Adult Income dataset to prepare it for machine learning.\n",
    "\n",
    "### **Handling Missing Values**\n",
    "\n",
    "The dataset contains some missing values, represented as `\"NaN\"`, particularly in categorical features like `workclass`, `occupation`, and `native-country`. We will address them using two strategies:\n",
    "\n",
    "- **Removing rows with missing values** to eliminate uncertainty.\n",
    "- **Replacing missing values** with the most frequent value (mode) in the respective column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "85fced8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: workclass\n",
      "  Most common value: Private\n",
      "  Frequency: 33906 (73.64% of non-missing values)\n",
      "\n",
      "Column: occupation\n",
      "  Most common value: Prof-specialty\n",
      "  Frequency: 6172 (13.41% of non-missing values)\n",
      "\n",
      "Column: native-country\n",
      "  Most common value: United-States\n",
      "  Frequency: 43832 (91.35% of non-missing values)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = ['workclass', 'occupation', 'native-country']\n",
    "\n",
    "for col in columns:\n",
    "    mode_value = df[col].mode()[0]\n",
    "    mode_count = df[col].value_counts().loc[mode_value]\n",
    "    total_non_missing = df[col].notna().sum()\n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\"  Most common value: {mode_value}\")\n",
    "    print(f\"  Frequency: {mode_count} ({mode_count / total_non_missing:.2%} of non-missing values)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718ddad8",
   "metadata": {},
   "source": [
    "### **Missing Values Chosen Strategy** \n",
    "\n",
    "Missing values in the workclass, occupation, and native-country columns were imputed using the most frequent value (mode) in each column. This strategy retains more data while introducing minimal bias, as the mode values were dominant — especially in native-country (>91%) and workclass (>73%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b7874876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of rows: 48842\n",
      "Remaining rows after dropping: 45222\n",
      "Rows dropped: 3620\n",
      "Percentage of data lost: 7.41%\n"
     ]
    }
   ],
   "source": [
    "original_rows = df.shape[0]\n",
    "\n",
    "# Drop rows with missing values in the selected columns\n",
    "df_dropped = df.dropna(subset=['workclass', 'occupation', 'native-country'])\n",
    "\n",
    "# Number of rows after dropping\n",
    "remaining_rows = df_dropped.shape[0]\n",
    "\n",
    "# Number of rows removed\n",
    "rows_dropped = original_rows - remaining_rows\n",
    "\n",
    "# Percentage of data lost\n",
    "percent_lost = (rows_dropped / original_rows) * 100\n",
    "\n",
    "print(f\"Original number of rows: {original_rows}\")\n",
    "print(f\"Remaining rows after dropping: {remaining_rows}\")\n",
    "print(f\"Rows dropped: {rows_dropped}\")\n",
    "print(f\"Percentage of data lost: {percent_lost:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a569cd",
   "metadata": {},
   "source": [
    "### **Dropping Rows with missing Values**\n",
    "\n",
    "The dataset contains missing values in the workclass, occupation, and native-country columns. Dropping rows with missing values would result in a loss of 7.41% of the data (3,620 rows). To preserve the dataset size and avoid potential bias introduced by data loss, we chose to impute missing values using the most frequent (mode) value for each column. The dominant presence of these mode values (e.g., native-country = \"United-States\" at 91%) supports this choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ed2da4",
   "metadata": {},
   "source": [
    "### **Dataset After This Step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c93e305d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after imputation:\n",
      "age               0\n",
      "workclass         0\n",
      "fnlwgt            0\n",
      "education         0\n",
      "education-num     0\n",
      "marital-status    0\n",
      "occupation        0\n",
      "relationship      0\n",
      "race              0\n",
      "sex               0\n",
      "capital-gain      0\n",
      "capital-loss      0\n",
      "hours-per-week    0\n",
      "native-country    0\n",
      "income            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "columns_to_impute = ['workclass', 'occupation', 'native-country']\n",
    "for col in columns_to_impute:\n",
    "    mode_val = df[col].mode()[0]\n",
    "    df[col] = df[col].fillna(mode_val)\n",
    "\n",
    "# Check if there are any missing values left\n",
    "missing_values_after = df.isna().sum()\n",
    "print(\"Missing values after imputation:\")\n",
    "print(missing_values_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ae8793",
   "metadata": {},
   "source": [
    "### **Encoding Categorical Features**\n",
    "\n",
    "Machine learning models typically require numerical input. Therefore, we will encode categorical features such as `education`, `marital-status`, and `occupation` using:\n",
    "\n",
    "- **One-hot encoding** for features with no ordinal relationship.\n",
    "- **Label encoding** for features with inherent order (if applicable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173151e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48842, 15)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "print(df.shape)pickle\n",
    "\n",
    "# One-hot encode nominal categorical features\n",
    "one_hot_cols = [\n",
    "    'workclass', 'marital-status', 'occupation', 'relationship',\n",
    "    'race', 'sex', 'native-country'\n",
    "]\n",
    "df = pd.get_dummies(df, columns=one_hot_cols, drop_first=False)\n",
    "\n",
    "# Drop the redundant 'education' column\n",
    "df.drop(columns='education', inplace=True)\n",
    "\n",
    "# Map income to binary target\n",
    "df['income'] = df['income'].map({'<=50K': 0, '>50K': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "30080a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>income</th>\n",
       "      <th>workclass_Federal-gov</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>workclass_Never-worked</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_Portugal</th>\n",
       "      <th>native-country_Puerto-Rico</th>\n",
       "      <th>native-country_Scotland</th>\n",
       "      <th>native-country_South</th>\n",
       "      <th>native-country_Taiwan</th>\n",
       "      <th>native-country_Thailand</th>\n",
       "      <th>native-country_Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_United-States</th>\n",
       "      <th>native-country_Vietnam</th>\n",
       "      <th>native-country_Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>77516</td>\n",
       "      <td>13</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>83311</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>215646</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>234721</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>338409</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  fnlwgt  education-num  capital-gain  capital-loss  hours-per-week  \\\n",
       "0   39   77516             13          2174             0              40   \n",
       "1   50   83311             13             0             0              13   \n",
       "2   38  215646              9             0             0              40   \n",
       "3   53  234721              7             0             0              40   \n",
       "4   28  338409             13             0             0              40   \n",
       "\n",
       "   income  workclass_Federal-gov  workclass_Local-gov  workclass_Never-worked  \\\n",
       "0       0                  False                False                   False   \n",
       "1       0                  False                False                   False   \n",
       "2       0                  False                False                   False   \n",
       "3       0                  False                False                   False   \n",
       "4       0                  False                False                   False   \n",
       "\n",
       "   ...  native-country_Portugal  native-country_Puerto-Rico  \\\n",
       "0  ...                    False                       False   \n",
       "1  ...                    False                       False   \n",
       "2  ...                    False                       False   \n",
       "3  ...                    False                       False   \n",
       "4  ...                    False                       False   \n",
       "\n",
       "   native-country_Scotland  native-country_South  native-country_Taiwan  \\\n",
       "0                    False                 False                  False   \n",
       "1                    False                 False                  False   \n",
       "2                    False                 False                  False   \n",
       "3                    False                 False                  False   \n",
       "4                    False                 False                  False   \n",
       "\n",
       "   native-country_Thailand  native-country_Trinadad&Tobago  \\\n",
       "0                    False                           False   \n",
       "1                    False                           False   \n",
       "2                    False                           False   \n",
       "3                    False                           False   \n",
       "4                    False                           False   \n",
       "\n",
       "   native-country_United-States  native-country_Vietnam  \\\n",
       "0                          True                   False   \n",
       "1                          True                   False   \n",
       "2                          True                   False   \n",
       "3                          True                   False   \n",
       "4                         False                   False   \n",
       "\n",
       "   native-country_Yugoslavia  \n",
       "0                      False  \n",
       "1                      False  \n",
       "2                      False  \n",
       "3                      False  \n",
       "4                      False  \n",
       "\n",
       "[5 rows x 90 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 5 rows to check encoding results\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775ff236",
   "metadata": {},
   "source": [
    "### **Enconding Resoning and Results**\n",
    "\n",
    "We encoded the categorical features to convert them into a numeric format suitable for machine learning algorithms. Since the education feature has a clear order of attainment levels, we applied label encoding based on its natural hierarchy, preserving the ordinal relationship. For other categorical features without inherent order—such as workclass, marital-status, occupation, relationship, race, sex, and native-country—we applied one-hot encoding, which created new binary columns for each category. This expanded the feature space but allowed models to interpret categorical data effectively. Lastly, the target variable income was binarized to 0 and 1, facilitating binary classification. This preprocessing step transformed all relevant columns into numeric form, enabling downstream model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "55bff2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48842, 90)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f24e945",
   "metadata": {},
   "source": [
    "### **Why Does The Data Change Shape**\n",
    "\n",
    "The dataset’s number of columns increases after encoding because one-hot encoding creates a new binary column for each category in the original categorical features. This expands the feature space to convert categorical data into a numeric format suitable for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1458a990",
   "metadata": {},
   "source": [
    "### **Transforming Numerical Features**\n",
    "\n",
    "Some continuous numerical features contain extreme or highly granular values that could introduce noise:\n",
    "\n",
    "- `hours-per-week` includes values as low as 1 and as high as 99 — we may **clip or bin these values** into broader categories (e.g., part-time, full-time, overtime).\n",
    "- `capital-gain` and `capital-loss` have **many rare and highly specific values**, most of which are zero — we may **bucketize or simplify** these into categories like \"none\", \"low\", and \"high\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1fa0ba95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top capital-gain values:\n",
      "capital-gain\n",
      "0        44807\n",
      "15024      513\n",
      "7688       410\n",
      "7298       364\n",
      "99999      244\n",
      "3103       152\n",
      "5178       146\n",
      "5013       117\n",
      "4386       108\n",
      "8614        82\n",
      "3325        81\n",
      "2174        74\n",
      "10520       64\n",
      "4650        63\n",
      "27828       58\n",
      "4064        54\n",
      "594         52\n",
      "3137        51\n",
      "14084       49\n",
      "20051       49\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top capital-loss values:\n",
      "capital-loss\n",
      "0       46560\n",
      "1902      304\n",
      "1977      253\n",
      "1887      233\n",
      "2415       72\n",
      "1485       71\n",
      "1848       67\n",
      "1590       62\n",
      "1602       62\n",
      "1876       59\n",
      "1740       58\n",
      "1672       50\n",
      "1741       44\n",
      "1564       43\n",
      "2258       39\n",
      "1719       38\n",
      "1980       36\n",
      "1408       35\n",
      "1669       35\n",
      "2001       35\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot capital-gain distribution (log scale to handle skewness)\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df['capital-gain'], bins=50, color='skyblue')\n",
    "plt.title('Capital Gain Distribution')\n",
    "plt.xlabel('Capital Gain')\n",
    "plt.ylabel('Frequency')\n",
    "plt.yscale('log')  # Log scale for better visibility of rare high values\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df['capital-loss'], bins=50, color='salmon')\n",
    "plt.title('Capital Loss Distribution')\n",
    "plt.xlabel('Capital Loss')\n",
    "plt.ylabel('Frequency')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Also, print value counts for the top values to see where to place threshold\n",
    "print(\"Top capital-gain values:\")\n",
    "print(df['capital-gain'].value_counts().head(20))\n",
    "\n",
    "print(\"\\nTop capital-loss values:\")\n",
    "print(df['capital-loss'].value_counts().head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7394c5f3",
   "metadata": {},
   "source": [
    "### **Binning Strategy**\n",
    "\n",
    "To simplify the numerical features and reduce noise from extreme values, we binned capital-gain and capital-loss into three categories: \"none\" for zero values, \"low\" for values up to a chosen threshold, and \"high\" for values above it. Based on data distribution analysis, we selected 7,000 as the threshold for capital-gain and 2,000 for capital-loss. Additionally, we transformed hours-per-week into categorical bins representing work intensity: \"part-time\" (≤30 hours), \"full-time\" (31–40 hours), and \"overtime\" (>40 hours). This binning strategy groups continuous values into meaningful categories, helping reduce noise and improving model interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2ebd5ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48842, 96)\n",
      "   age  fnlwgt  education-num  income  workclass_Federal-gov  \\\n",
      "0   39   77516             13       0                  False   \n",
      "1   50   83311             13       0                  False   \n",
      "2   38  215646              9       0                  False   \n",
      "3   53  234721              7       0                  False   \n",
      "4   28  338409             13       0                  False   \n",
      "\n",
      "   workclass_Local-gov  workclass_Never-worked  workclass_Private  \\\n",
      "0                False                   False              False   \n",
      "1                False                   False              False   \n",
      "2                False                   False               True   \n",
      "3                False                   False               True   \n",
      "4                False                   False               True   \n",
      "\n",
      "   workclass_Self-emp-inc  workclass_Self-emp-not-inc  ...  \\\n",
      "0                   False                       False  ...   \n",
      "1                   False                        True  ...   \n",
      "2                   False                       False  ...   \n",
      "3                   False                       False  ...   \n",
      "4                   False                       False  ...   \n",
      "\n",
      "   native-country_Yugoslavia  hours-per-week-binned_full-time  \\\n",
      "0                      False                             True   \n",
      "1                      False                            False   \n",
      "2                      False                             True   \n",
      "3                      False                             True   \n",
      "4                      False                             True   \n",
      "\n",
      "   hours-per-week-binned_overtime  hours-per-week-binned_part-time  \\\n",
      "0                           False                            False   \n",
      "1                           False                             True   \n",
      "2                           False                            False   \n",
      "3                           False                            False   \n",
      "4                           False                            False   \n",
      "\n",
      "   capital-gain-binned_high  capital-gain-binned_low  \\\n",
      "0                     False                     True   \n",
      "1                     False                    False   \n",
      "2                     False                    False   \n",
      "3                     False                    False   \n",
      "4                     False                    False   \n",
      "\n",
      "   capital-gain-binned_none  capital-loss-binned_high  \\\n",
      "0                     False                     False   \n",
      "1                      True                     False   \n",
      "2                      True                     False   \n",
      "3                      True                     False   \n",
      "4                      True                     False   \n",
      "\n",
      "   capital-loss-binned_low  capital-loss-binned_none  \n",
      "0                    False                      True  \n",
      "1                    False                      True  \n",
      "2                    False                      True  \n",
      "3                    False                      True  \n",
      "4                    False                      True  \n",
      "\n",
      "[5 rows x 96 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Copy original dataframe\n",
    "df_transformed = df.copy()\n",
    "\n",
    "# Binning hours-per-week\n",
    "def hours_bin(hours):\n",
    "    if hours <= 30:\n",
    "        return 'part-time'\n",
    "    elif hours <= 40:\n",
    "        return 'full-time'\n",
    "    else:\n",
    "        return 'overtime'\n",
    "\n",
    "df_transformed['hours-per-week-binned'] = df_transformed['hours-per-week'].apply(hours_bin)\n",
    "\n",
    "# Binning capital-gain\n",
    "def capital_gain_bin(gain):\n",
    "    if gain == 0:\n",
    "        return 'none'\n",
    "    elif gain <= 7000:\n",
    "        return 'low'\n",
    "    else:\n",
    "        return 'high'\n",
    "\n",
    "df_transformed['capital-gain-binned'] = df_transformed['capital-gain'].apply(capital_gain_bin)\n",
    "\n",
    "# Binning capital-loss\n",
    "def capital_loss_bin(loss):\n",
    "    if loss == 0:\n",
    "        return 'none'\n",
    "    elif loss <= 2000:\n",
    "        return 'low'\n",
    "    else:\n",
    "        return 'high'\n",
    "\n",
    "df_transformed['capital-loss-binned'] = df_transformed['capital-loss'].apply(capital_loss_bin)\n",
    "\n",
    "# Drop original continuous columns\n",
    "df_transformed = df_transformed.drop(columns=['hours-per-week', 'capital-gain', 'capital-loss'])\n",
    "\n",
    "# One-hot encode the new binned columns\n",
    "binned_cols = ['hours-per-week-binned', 'capital-gain-binned', 'capital-loss-binned']\n",
    "df_transformed = pd.get_dummies(df_transformed, columns=binned_cols)\n",
    "# Check results\n",
    "print(df_transformed.shape)\n",
    "print(df_transformed.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18171560",
   "metadata": {},
   "source": [
    "### **2 Versions Of Our Dataset**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1beb2f",
   "metadata": {},
   "source": [
    "This step of transforming numerical features into binned categories was considered optional. To evaluate its impact on model performance, we decided to create and retain two versions of the dataset: one with the original continuous numerical features, and another with the binned and encoded versions. This approach allows for a direct comparison to determine which preprocessing strategy yields better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "61f7232d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame shape: (48842, 90)\n"
     ]
    }
   ],
   "source": [
    "df1 = df.copy()\n",
    "df2 = df_transformed.copy()\n",
    "\n",
    "print(\"Original DataFrame shape:\", df1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5036be56",
   "metadata": {},
   "source": [
    "### **Splitting the Data**\n",
    "\n",
    "We will explore one approaches to splitting the dataset for training and testing:\n",
    "\n",
    "- Perform **random train/test splits** (e.g., 80/20 or 70/30) using `train_test_split` for experimentation and validation.\n",
    "\n",
    "### **Optional: Addressing Class Imbalance**\n",
    "\n",
    "If we find that the dataset is imbalanced (e.g., far more samples with income ≤50K than >50K), we may apply techniques to balance the classes:\n",
    "\n",
    "- **Oversampling** the minority class.\n",
    "- **Undersampling** the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9224326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df1 and df2 include features + target 'income'\n",
    "X1, y1 = df1.drop(columns='income'), df1['income']\n",
    "X2, y2 = df2.drop(columns='income'), df2['income']\n",
    "\n",
    "def generate_splits(X, y, n_splits=10, test_size=0.2, random_state=42):\n",
    "    raw_splits = []\n",
    "    over_splits = []\n",
    "    under_splits = []\n",
    "    \n",
    "    ros = RandomOverSampler(random_state=random_state)\n",
    "    rus = RandomUnderSampler(random_state=random_state)\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        rs = random_state + i  # change seed for diversity\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, stratify=y, random_state=rs\n",
    "        )\n",
    "        \n",
    "        # Raw (no sampling)\n",
    "        raw_splits.append((X_train, X_test, y_train, y_test))\n",
    "        \n",
    "        # Oversampling only\n",
    "        X_over, y_over = ros.fit_resample(X_train, y_train)\n",
    "        over_splits.append((X_over, X_test, y_over, y_test))\n",
    "        \n",
    "        # Undersampling only\n",
    "        X_under, y_under = rus.fit_resample(X_train, y_train)\n",
    "        under_splits.append((X_under, X_test, y_under, y_test))\n",
    "        \n",
    "    \n",
    "    return raw_splits, over_splits, under_splits\n",
    "\n",
    "# Generate splits for df1\n",
    "df1_raw, df1_OverSampling, df1_UnderSampling = generate_splits(X1, y1)\n",
    "\n",
    "# Generate splits for df2\n",
    "df2_raw, df2_OverSampling, df2_UnderSampling = generate_splits(X2, y2)\n",
    "\n",
    "# Example of accessing a split:\n",
    "# X_train, X_test, y_train, y_test = df1_OverSampling[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734ed224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of first split for df1 (raw): (9769, 89)\n",
      "Shape of first split for df1 (oversampled): (59448, 89)\n",
      "Shape of first split for df1 (undersampled): (18698, 89)\n",
      "Shape of first split for df2 (raw): (39073, 95)\n",
      "Shape of first split for df2 (oversampled): (59448, 95)\n",
      "Shape of first split for df2 (undersampled): (18698, 95)\n"
     ]
    }
   ],
   "source": [
    "#printing the shape of the first split for df1\n",
    "print(\"Shape of first split for df1 (raw):\", df1_raw[0][0].shape)\n",
    "print(\"Shape of first split for df1 (oversampled):\", df1_OverSampling[0][0].shape)\n",
    "print(\"Shape of first split for df1 (undersampled):\", df1_UnderSampling[0][0].shape)\n",
    "# Printing the shape of the first split for df2\n",
    "print(\"Shape of first split for df2 (raw):\", df2_raw[0][0].shape)\n",
    "print(\"Shape of first split for df2 (oversampled):\", df2_OverSampling[0][0].shape)\n",
    "print(\"Shape of first split for df2 (undersampled):\", df2_UnderSampling[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee69b64",
   "metadata": {},
   "source": [
    "### **Splitting and sampling Strategy**\n",
    "\n",
    "\n",
    "We created multiple random 80/20 train-test splits for each dataset version to ensure robust evaluation across different data partitions. To address the class imbalance present in the income target variable, we generated splits under four conditions:\n",
    "\n",
    "Raw data with no sampling applied,\n",
    "\n",
    "Oversampling of the minority class,\n",
    "\n",
    "Undersampling of the majority class, and\n",
    "\n",
    "Stratified splitting was used to maintain the original class distribution in each split, ensuring consistent representation of both income classes during training and testing. This approach allows us to compare the impact of different sampling techniques on model performance more reliably."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc1a763",
   "metadata": {},
   "source": [
    "## **Model Implementation**\n",
    "\n",
    "In this section, we will implement and evaluate various machine learning models to predict whether an individual's income exceeds $50K based on the features in the dataset. The objective is to confirm and apply machine learning theory learned in class, deepen our understanding of classification models, and identify the most effective approach for this problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f160bd",
   "metadata": {},
   "source": [
    "### **Evaluation Metrics**\n",
    "\n",
    "All models will be evaluated using key classification metrics to gain a comprehensive view of performance:\n",
    "\n",
    "- **Accuracy**  \n",
    "  The proportion of total predictions that are correct. Good for balanced datasets.\n",
    "\n",
    "- **Precision**  \n",
    "  The proportion of positive predictions that were actually correct. Important when false positives are costly.\n",
    "\n",
    "- **Recall**  \n",
    "  The proportion of actual positives that were correctly predicted. Important when false negatives are costly.\n",
    "\n",
    "- **F1-Score**  \n",
    "  The harmonic mean of precision and recall. Useful when you need a balance between precision and recall.\n",
    "\n",
    "- **ROC AUC Score**  \n",
    "  Measures the ability of the model to distinguish between classes across different thresholds. A higher score indicates better overall classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5508e062",
   "metadata": {},
   "source": [
    "\n",
    "### **Implemented Models**\n",
    "\n",
    "We will start by implementing the following classification models:\n",
    "\n",
    "- **Logistic Regression**  \n",
    "  A simple and interpretable linear model; it's a good baseline for binary classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "73e7e832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Logistic Regression Performance (mean ± std):\n",
      "\n",
      "                          accuracy        precision           recall               f1          roc_auc\n",
      "dataset                                                                                               \n",
      "df1_OverSampling   0.8055 ± 0.0042  0.5624 ± 0.0062  0.8442 ± 0.0094   0.675 ± 0.0064  0.9033 ± 0.0031\n",
      "df1_UnderSampling  0.8045 ± 0.0047  0.5609 ± 0.0072   0.8441 ± 0.009   0.674 ± 0.0063   0.9031 ± 0.003\n",
      "df1_raw            0.8503 ± 0.0035  0.7288 ± 0.0105  0.5967 ± 0.0089  0.6561 ± 0.0078  0.9036 ± 0.0031\n",
      "df2_OverSampling   0.8104 ± 0.0036  0.5702 ± 0.0055   0.8448 ± 0.008  0.6809 ± 0.0054  0.9076 ± 0.0029\n",
      "df2_UnderSampling   0.8098 ± 0.004  0.5691 ± 0.0062  0.8459 ± 0.0087  0.6804 ± 0.0056  0.9075 ± 0.0029\n",
      "df2_raw            0.8545 ± 0.0034   0.747 ± 0.0116  0.5932 ± 0.0083  0.6612 ± 0.0075  0.9078 ± 0.0028\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# List of dataset split arrays (you must already have these defined)\n",
    "datasets = {\n",
    "    'df1_raw': df1_raw,\n",
    "    'df1_OverSampling': df1_OverSampling,\n",
    "    'df1_UnderSampling': df1_UnderSampling,\n",
    "    'df2_raw': df2_raw,\n",
    "    'df2_OverSampling': df2_OverSampling,\n",
    "    'df2_UnderSampling': df2_UnderSampling,\n",
    "}\n",
    "\n",
    "# Initialize empty list to collect all results\n",
    "results = []\n",
    "\n",
    "for name, splits in datasets.items():\n",
    "    \n",
    "    for i, (X_train, X_test, y_train, y_test) in enumerate(splits):\n",
    "        \n",
    "        # Create pipeline with scaling\n",
    "        model = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
    "\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Compute metrics\n",
    "        metrics = {\n",
    "            'dataset': name,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "        }\n",
    "\n",
    "        results.append(metrics)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Group by dataset and compute mean and std\n",
    "grouped = results_df.groupby('dataset').agg(['mean', 'std']).round(4)\n",
    "\n",
    "# Create a new DataFrame to store \"mean ± std\" strings\n",
    "formatted = pd.DataFrame(index=grouped.index)\n",
    "\n",
    "# Loop through metric columns\n",
    "for metric in grouped.columns.levels[0]:\n",
    "    mean_col = (metric, 'mean')\n",
    "    std_col = (metric, 'std')\n",
    "    \n",
    "    formatted[metric] = grouped[mean_col].astype(str) + ' ± ' + grouped[std_col].astype(str)\n",
    "\n",
    "# Print pretty table\n",
    "print(\"\\n📊 Logistic Regression Performance (mean ± std):\\n\")\n",
    "print(formatted.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9677717a",
   "metadata": {},
   "source": [
    "📊 Logistic Regression Performance (mean ± std):\n",
    "\n",
    "                          accuracy        precision           recall               f1          roc_auc\n",
    "dataset                                                                                               \n",
    "df1_OverSampling   0.8055 ± 0.0042  0.5624 ± 0.0062  0.8442 ± 0.0094   0.675 ± 0.0064  0.9033 ± 0.0031\n",
    "df1_UnderSampling  0.8045 ± 0.0047  0.5609 ± 0.0072   0.8441 ± 0.009   0.674 ± 0.0063   0.9031 ± 0.003\n",
    "df1_raw            0.8503 ± 0.0035  0.7288 ± 0.0105  0.5967 ± 0.0089  0.6561 ± 0.0078  0.9036 ± 0.0031\n",
    "df2_OverSampling   0.8104 ± 0.0036  0.5702 ± 0.0055   0.8448 ± 0.008  0.6809 ± 0.0054  0.9076 ± 0.0029\n",
    "df2_UnderSampling   0.8098 ± 0.004  0.5691 ± 0.0062  0.8459 ± 0.0087  0.6804 ± 0.0056  0.9075 ± 0.0029\n",
    "df2_raw            0.8545 ± 0.0034   0.747 ± 0.0116  0.5932 ± 0.0083  0.6612 ± 0.0075  0.9078 ± 0.0028"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222fe74",
   "metadata": {},
   "source": [
    "### **Conclusoes**\n",
    "\n",
    "falar do que se fez \n",
    "falar da pipeline e dizer que diminui muito o tempo de treino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d9b1fe",
   "metadata": {},
   "source": [
    "- **Random Forest Classifier**  \n",
    "  A robust ensemble method that reduces overfitting; chosen for its ability to handle non-linear relationships and mixed feature types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9f421044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Random Forest model training and evaluation.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "skip = True\n",
    "if skip:\n",
    "    print(\"Skipping Random Forest model training and evaluation.\")\n",
    "else:\n",
    "    # List of dataset split arrays (already defined)\n",
    "    datasets = {\n",
    "        'df1_raw': df1_raw,\n",
    "        'df1_OverSampling': df1_OverSampling,\n",
    "        'df1_UnderSampling': df1_UnderSampling,\n",
    "        'df2_raw': df2_raw,\n",
    "        'df2_OverSampling': df2_OverSampling,\n",
    "        'df2_UnderSampling': df2_UnderSampling,\n",
    "    }\n",
    "\n",
    "    # Initialize empty list to collect all results\n",
    "    rf_results = []\n",
    "\n",
    "    for name, splits in datasets.items():\n",
    "        \n",
    "        for i, (X_train, X_test, y_train, y_test) in enumerate(splits):\n",
    "\n",
    "            # Initialize Random Forest model\n",
    "            model =  RandomForestClassifier(random_state=42, n_estimators= 100)\n",
    "\n",
    "            # Train model\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Predict\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            # Compute metrics\n",
    "            metrics = {\n",
    "                'dataset': name,\n",
    "                'accuracy': accuracy_score(y_test, y_pred),\n",
    "                'precision': precision_score(y_test, y_pred),\n",
    "                'recall': recall_score(y_test, y_pred),\n",
    "                'f1': f1_score(y_test, y_pred),\n",
    "                'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "            }\n",
    "\n",
    "            rf_results.append(metrics)\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    rf_results_df = pd.DataFrame(rf_results)\n",
    "\n",
    "    # Group by dataset and compute mean and std\n",
    "    grouped = rf_results_df.groupby('dataset').agg(['mean', 'std']).round(4)\n",
    "\n",
    "    # Create a new DataFrame to store \"mean ± std\" strings\n",
    "    formatted = pd.DataFrame(index=grouped.index)\n",
    "\n",
    "    # Loop through metric columns\n",
    "    for metric in grouped.columns.levels[0]:\n",
    "        mean_col = (metric, 'mean')\n",
    "        std_col = (metric, 'std')\n",
    "        \n",
    "        formatted[metric] = grouped[mean_col].astype(str) + ' ± ' + grouped[std_col].astype(str)\n",
    "\n",
    "    # Print pretty table\n",
    "    print(\"\\n📊 Random Forest Performance (mean ± std):\\n\")\n",
    "    print(formatted.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "555c8fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Random Forest model training and evaluation.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "skip = True\n",
    "if skip:\n",
    "    print(\"Skipping Random Forest model training and evaluation.\")\n",
    "else:\n",
    "\n",
    "    # List of dataset split arrays (already defined)\n",
    "    datasets = {\n",
    "        'df1_raw': df1_raw,\n",
    "        'df1_OverSampling': df1_OverSampling,\n",
    "        'df1_UnderSampling': df1_UnderSampling,\n",
    "        'df2_raw': df2_raw,\n",
    "        'df2_OverSampling': df2_OverSampling,\n",
    "        'df2_UnderSampling': df2_UnderSampling,\n",
    "    }\n",
    "\n",
    "    # Initialize empty list to collect all results\n",
    "    rf_results = []\n",
    "\n",
    "    for name, splits in datasets.items():\n",
    "        \n",
    "        for i, (X_train, X_test, y_train, y_test) in enumerate(splits):\n",
    "\n",
    "            # Initialize Random Forest model\n",
    "            model = RandomForestClassifier(random_state=42, n_estimators= 100)\n",
    "\n",
    "            # Train model\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Predict\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            # Compute metrics\n",
    "            metrics = {\n",
    "                'dataset': name,\n",
    "                'accuracy': accuracy_score(y_test, y_pred),\n",
    "                'precision': precision_score(y_test, y_pred, pos_label=0),\n",
    "                'recall': recall_score(y_test, y_pred, pos_label=0),\n",
    "                'f1': f1_score(y_test, y_pred, pos_label=0),\n",
    "                'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "            }\n",
    "\n",
    "            rf_results.append(metrics)\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    rf_results_df = pd.DataFrame(rf_results)\n",
    "\n",
    "    # Group by dataset and compute mean and std\n",
    "    grouped = rf_results_df.groupby('dataset').agg(['mean', 'std']).round(4)\n",
    "\n",
    "    # Create a new DataFrame to store \"mean ± std\" strings\n",
    "    formatted = pd.DataFrame(index=grouped.index)\n",
    "\n",
    "    # Loop through metric columns\n",
    "    for metric in grouped.columns.levels[0]:\n",
    "        mean_col = (metric, 'mean')\n",
    "        std_col = (metric, 'std')\n",
    "        \n",
    "        formatted[metric] = grouped[mean_col].astype(str) + ' ± ' + grouped[std_col].astype(str)\n",
    "\n",
    "    # Print pretty table\n",
    "    print(\"\\n📊 Random Forest Performance for Class 0 (mean ± std):\\n\")\n",
    "    print(formatted.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fd0a8e",
   "metadata": {},
   "source": [
    "### **Conclusoes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ca133c",
   "metadata": {},
   "source": [
    "- **Gradient Boosting (XGBoost or LightGBM)**  \n",
    "  A highly accurate boosting algorithm; selected for its strong performance in structured/tabular data and ability to capture complex patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "80cb3580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 XGBoost Performance (mean ± std):\n",
      "\n",
      "                          accuracy        precision           recall               f1          roc_auc\n",
      "dataset                                                                                               \n",
      "df1_OverSampling   0.8356 ± 0.0031  0.6132 ± 0.0053  0.8482 ± 0.0078  0.7118 ± 0.0049  0.9267 ± 0.0031\n",
      "df1_UnderSampling  0.8269 ± 0.0035   0.5951 ± 0.006  0.8663 ± 0.0077  0.7055 ± 0.0049  0.9251 ± 0.0033\n",
      "df1_raw            0.8717 ± 0.0042  0.7751 ± 0.0119  0.6537 ± 0.0131  0.7092 ± 0.0101  0.9275 ± 0.0036\n",
      "df2_OverSampling   0.8195 ± 0.0037  0.5866 ± 0.0064  0.8328 ± 0.0077  0.6883 ± 0.0048   0.911 ± 0.0028\n",
      "df2_UnderSampling  0.8096 ± 0.0035  0.5682 ± 0.0057  0.8518 ± 0.0057  0.6817 ± 0.0039  0.9102 ± 0.0027\n",
      "df2_raw            0.8578 ± 0.0029   0.7429 ± 0.009  0.6207 ± 0.0107  0.6763 ± 0.0074  0.9123 ± 0.0028\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# List of dataset split arrays (already defined)\n",
    "datasets = {\n",
    "    'df1_raw': df1_raw,\n",
    "    'df1_OverSampling': df1_OverSampling,\n",
    "    'df1_UnderSampling': df1_UnderSampling,\n",
    "    'df2_raw': df2_raw,\n",
    "    'df2_OverSampling': df2_OverSampling,\n",
    "    'df2_UnderSampling': df2_UnderSampling,\n",
    "}\n",
    "\n",
    "# Initialize list to collect results\n",
    "xgb_results = []\n",
    "\n",
    "for name, splits in datasets.items():\n",
    "    \n",
    "    for i, (X_train, X_test, y_train, y_test) in enumerate(splits):\n",
    "\n",
    "        # Initialize XGBoost model\n",
    "        model =  XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "\n",
    "        # Train\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Metrics\n",
    "        metrics = {\n",
    "            'dataset': name,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "        }\n",
    "\n",
    "        xgb_results.append(metrics)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "xgb_results_df = pd.DataFrame(xgb_results)\n",
    "\n",
    "# Group by dataset and compute mean and std\n",
    "grouped = xgb_results_df.groupby('dataset').agg(['mean', 'std']).round(4)\n",
    "\n",
    "# Create a new DataFrame to store \"mean ± std\" strings\n",
    "formatted = pd.DataFrame(index=grouped.index)\n",
    "\n",
    "# Loop through metric columns\n",
    "for metric in grouped.columns.levels[0]:\n",
    "    mean_col = (metric, 'mean')\n",
    "    std_col = (metric, 'std')\n",
    "    \n",
    "    formatted[metric] = grouped[mean_col].astype(str) + ' ± ' + grouped[std_col].astype(str)\n",
    "\n",
    "# Print pretty table\n",
    "print(\"\\n📊 XGBoost Performance (mean ± std):\\n\")\n",
    "print(formatted.to_string())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "85d56a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 XGBoost Performance for class 0 (mean ± std):\n",
      "\n",
      "                          accuracy        precision           recall               f1          roc_auc\n",
      "dataset                                                                                               \n",
      "df1_OverSampling   0.8356 ± 0.0031  0.9457 ± 0.0026  0.8317 ± 0.0037   0.885 ± 0.0023  0.9267 ± 0.0031\n",
      "df1_UnderSampling  0.8269 ± 0.0035  0.9509 ± 0.0026  0.8145 ± 0.0048  0.8774 ± 0.0027  0.9251 ± 0.0033\n",
      "df1_raw            0.8717 ± 0.0042  0.8962 ± 0.0035   0.9403 ± 0.004  0.9177 ± 0.0027  0.9275 ± 0.0036\n",
      "df2_OverSampling   0.8195 ± 0.0037  0.9394 ± 0.0025  0.8153 ± 0.0054   0.873 ± 0.0029   0.911 ± 0.0028\n",
      "df2_UnderSampling  0.8096 ± 0.0035  0.9447 ± 0.0019  0.7963 ± 0.0053  0.8642 ± 0.0029  0.9102 ± 0.0027\n",
      "df2_raw            0.8578 ± 0.0029  0.8866 ± 0.0027  0.9324 ± 0.0034  0.9089 ± 0.0019  0.9123 ± 0.0028\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# List of dataset split arrays (already defined)\n",
    "datasets = {\n",
    "    'df1_raw': df1_raw,\n",
    "    'df1_OverSampling': df1_OverSampling,\n",
    "    'df1_UnderSampling': df1_UnderSampling,\n",
    "    'df2_raw': df2_raw,\n",
    "    'df2_OverSampling': df2_OverSampling,\n",
    "    'df2_UnderSampling': df2_UnderSampling,\n",
    "}\n",
    "\n",
    "# Initialize list to collect results\n",
    "xgb_results = []\n",
    "\n",
    "for name, splits in datasets.items():\n",
    "    \n",
    "    for i, (X_train, X_test, y_train, y_test) in enumerate(splits):\n",
    "\n",
    "        # Initialize XGBoost model\n",
    "        model =  XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "\n",
    "        # Train\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Metrics\n",
    "        metrics = {\n",
    "            'dataset': name,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred, pos_label=0),\n",
    "            'recall': recall_score(y_test, y_pred, pos_label=0),\n",
    "            'f1': f1_score(y_test, y_pred, pos_label=0),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "        }\n",
    "\n",
    "        xgb_results.append(metrics)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "xgb_results_df = pd.DataFrame(xgb_results)\n",
    "\n",
    "# Group by dataset and compute mean and std\n",
    "grouped = xgb_results_df.groupby('dataset').agg(['mean', 'std']).round(4)\n",
    "\n",
    "# Create a new DataFrame to store \"mean ± std\" strings\n",
    "formatted = pd.DataFrame(index=grouped.index)\n",
    "\n",
    "# Loop through metric columns\n",
    "for metric in grouped.columns.levels[0]:\n",
    "    mean_col = (metric, 'mean')\n",
    "    std_col = (metric, 'std')\n",
    "    \n",
    "    formatted[metric] = grouped[mean_col].astype(str) + ' ± ' + grouped[std_col].astype(str)\n",
    "\n",
    "# Print pretty table\n",
    "print(\"\\n📊 XGBoost Performance for class 0 (mean ± std):\\n\")\n",
    "print(formatted.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9020f7",
   "metadata": {},
   "source": [
    "### **Conclusoes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a828035",
   "metadata": {},
   "source": [
    "### **Hyperparameter Tuning and Model Selection**\n",
    "\n",
    "To improve model performance, we may apply:\n",
    "\n",
    "- **Cross-validation**  \n",
    "  Using k-fold cross-validation to evaluate model reliability across different data splits.\n",
    "\n",
    "- **Grid Search or Randomized Search**  \n",
    "  For systematic tuning of hyperparameters, especially in Random Forest and Gradient Boosting models.\n",
    "\n",
    "As an enhancement, we performed hyperparameter tuning using GridSearchCV for Random Forest and RandomizedSearchCV for XGBoost. This process involved k-fold cross-validation to systematically evaluate model performance across different parameter settings. The objective was to identify configurations that improve classification metrics such as F1-score and ROC AUC, thereby strengthening the model's generalization capability. Once optimal parameters were found, the models were re-evaluated to confirm the improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fc50bf",
   "metadata": {},
   "source": [
    "### **Hyperparameter Tuning**\n",
    "\n",
    "\n",
    "Bank – High \t>=50K (1) precision         xgboost         df1_raw            0.8717 ± 0.0042  0.7751 ± 0.0119  0.6537 ± 0.0131  0.7092 ± 0.0101  0.9275 ± 0.0036\n",
    "\n",
    "IRS investigation       >=50K (1) recall    xgboost         df1_UnderSampling  0.8269 ± 0.0035   0.5951 ± 0.006  0.8663 ± 0.0077  0.7055 ± 0.0049  0.9251 ± 0.0033\n",
    "\n",
    "Gov Aid                 <50k (0) recall     xgboost         df1_raw              0.8711     0.8956  0.9401  0.9173   0.9275\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fdf99646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "# Define parameter grid for XGBoost\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Common cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548be930",
   "metadata": {},
   "source": [
    "### TUNING FOR PRECISION CLASS 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bcf72e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Best parameters for Precision: {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "✅ Best Precision from CV: 0.9846742351216274\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = df1_raw[0]\n",
    "\n",
    "grid_search_precision = GridSearchCV(\n",
    "    estimator=XGBClassifier(eval_metric='logloss', random_state=42),\n",
    "    param_grid=xgb_param_grid,\n",
    "    cv=cv,\n",
    "    scoring='precision',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "grid_search_precision.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n🎯 Best parameters for Precision:\", grid_search_precision.best_params_)\n",
    "print(\"✅ Best Precision from CV:\", grid_search_precision.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "0dd2b7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Precision Tune: df1_raw            0.8717 ± 0.0042  0.7751 ± 0.0119  0.6537 ± 0.0131  0.7092 ± 0.0101  0.9275 ± 0.0036\n",
      "\n",
      "📊 XGBoost Performance (mean ± std):\n",
      "\n",
      "                accuracy        precision           recall               f1          roc_auc\n",
      "dataset                                                                                     \n",
      "df1_raw  0.8161 ± 0.0023  0.9819 ± 0.0031  0.2361 ± 0.0099  0.3806 ± 0.0128  0.8905 ± 0.0033\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# List of dataset split arrays (already defined)\n",
    "datasets = {\n",
    "    'df1_raw': df1_raw\n",
    "}\n",
    "\n",
    "# Initialize list to collect results\n",
    "xgb_results = []\n",
    "\n",
    "for name, splits in datasets.items():\n",
    "    \n",
    "    for i, (X_train, X_test, y_train, y_test) in enumerate(splits):\n",
    "\n",
    "        # Initialize XGBoost model\n",
    "        model = XGBClassifier(eval_metric='logloss', random_state=42, colsample_bytree=1.0, learning_rate=0.01, max_depth= 3, n_estimators= 100, subsample= 1.0)\n",
    "\n",
    "        # Train\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Metrics\n",
    "        metrics = {\n",
    "            'dataset': name,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "        }\n",
    "\n",
    "        # Collect feature importances\n",
    "        feature_importances = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "        feature_importances.name = f\"{name}_split_{i}\"\n",
    "        xgb_results.append(metrics)\n",
    "        # Save importance separately\n",
    "        if 'feature_importance_bank' not in locals():\n",
    "            feature_importance_bank = []\n",
    "        feature_importance_bank.append(feature_importances)\n",
    "\n",
    "\n",
    "        xgb_results.append(metrics)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "xgb_results_df = pd.DataFrame(xgb_results)\n",
    "\n",
    "# Group by dataset and compute mean and std\n",
    "grouped = xgb_results_df.groupby('dataset').agg(['mean', 'std']).round(4)\n",
    "\n",
    "# Create a new DataFrame to store \"mean ± std\" strings\n",
    "formatted = pd.DataFrame(index=grouped.index)\n",
    "\n",
    "# Loop through metric columns\n",
    "for metric in grouped.columns.levels[0]:\n",
    "    mean_col = (metric, 'mean')\n",
    "    std_col = (metric, 'std')\n",
    "    \n",
    "    formatted[metric] = grouped[mean_col].astype(str) + ' ± ' + grouped[std_col].astype(str)\n",
    "\n",
    "# Save this model in a pickle file\n",
    "import pickle\n",
    "# Save model and feature names together\n",
    "model_package = {\n",
    "    'model': model,\n",
    "    'feature_names': model.feature_names_in_.tolist()\n",
    "}\n",
    "\n",
    "with open('xgb_model_precision_tuned.pkl', 'wb') as f:\n",
    "    pickle.dump(model_package, f)\n",
    "\n",
    "# Save feature importances to a DataFrame\n",
    "\n",
    "# Print pretty table\n",
    "print(\"Before Precision Tune: df1_raw            0.8717 ± 0.0042  0.7751 ± 0.0119  0.6537 ± 0.0131  0.7092 ± 0.0101  0.9275 ± 0.0036\")\n",
    "print(\"\\n📊 XGBoost Performance (mean ± std):\\n\")\n",
    "print(formatted.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87849f5",
   "metadata": {},
   "source": [
    "### TUNING FOR RECALL OF CLASS 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fe0438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Best parameters for Precision: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0}\n",
      "✅ Best Precision from CV: 0.8784884822161754\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = df1_UnderSampling[0]\n",
    "\n",
    "grid_search_precision = GridSearchCV(\n",
    "    estimator=XGBClassifier(eval_metric='logloss', random_state=42),\n",
    "    param_grid=xgb_param_grid,\n",
    "    cv=cv,\n",
    "    scoring='recall',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "grid_search_precision.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n🎯 Best parameters for Recall:\", grid_search_precision.best_params_)\n",
    "print(\"✅ Best Recall from CV:\", grid_search_precision.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "dbe29124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Recall Tune: df1_UnderSampling  0.8269 ± 0.0035   0.5951 ± 0.006  0.8663 ± 0.0077  0.7055 ± 0.0049  0.9251 ± 0.0033\n",
      "\n",
      "📊 XGBoost Performance (mean ± std):\n",
      "\n",
      "                          accuracy        precision         recall              f1         roc_auc\n",
      "dataset                                                                                           \n",
      "df1_UnderSampling  0.7789 ± 0.0066  0.5228 ± 0.0089  0.88 ± 0.0103  0.6558 ± 0.006  0.902 ± 0.0031\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# List of dataset split arrays (already defined)\n",
    "datasets = {\n",
    "    'df1_UnderSampling': df1_UnderSampling,\n",
    "}\n",
    "\n",
    "# Initialize list to collect results\n",
    "xgb_results = []\n",
    "\n",
    "for name, splits in datasets.items():\n",
    "    \n",
    "    for i, (X_train, X_test, y_train, y_test) in enumerate(splits):\n",
    "\n",
    "        # Initialize XGBoost model\n",
    "        model = XGBClassifier(eval_metric='logloss', random_state=42, colsample_bytree=0.8, learning_rate=0.01, max_depth= 3, n_estimators= 200, subsample= 1.0)\n",
    "\n",
    "        # Train\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Metrics\n",
    "        metrics = {\n",
    "            'dataset': name,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "        }\n",
    "\n",
    "        # Collect feature importances\n",
    "        feature_importances = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "        feature_importances.name = f\"{name}_split_{i}\"\n",
    "        xgb_results.append(metrics)\n",
    "        # Save importance separately\n",
    "        if 'feature_importance_irs' not in locals():\n",
    "            feature_importance_irs = []\n",
    "        feature_importance_irs.append(feature_importances)\n",
    "\n",
    "        xgb_results.append(metrics)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "xgb_results_df = pd.DataFrame(xgb_results)\n",
    "\n",
    "# Group by dataset and compute mean and std\n",
    "grouped = xgb_results_df.groupby('dataset').agg(['mean', 'std']).round(4)\n",
    "\n",
    "# Create a new DataFrame to store \"mean ± std\" strings\n",
    "formatted = pd.DataFrame(index=grouped.index)\n",
    "\n",
    "# Loop through metric columns\n",
    "for metric in grouped.columns.levels[0]:\n",
    "    mean_col = (metric, 'mean')\n",
    "    std_col = (metric, 'std')\n",
    "    \n",
    "    formatted[metric] = grouped[mean_col].astype(str) + ' ± ' + grouped[std_col].astype(str)\n",
    "\n",
    "# Save this model in a pickle file\n",
    "import pickle\n",
    "# Save model and feature names together\n",
    "model_package = {\n",
    "    'model': model,\n",
    "    'feature_names': model.feature_names_in_.tolist()\n",
    "}\n",
    "\n",
    "with open('xgb_model_recall_tuned.pkl', 'wb') as f:\n",
    "    pickle.dump(model_package, f)\n",
    "\n",
    "\n",
    "# Print pretty table\n",
    "print(\"Before Recall Tune: df1_UnderSampling  0.8269 ± 0.0035   0.5951 ± 0.006  0.8663 ± 0.0077  0.7055 ± 0.0049  0.9251 ± 0.0033\")\n",
    "print(\"\\n📊 XGBoost Performance (mean ± std):\\n\")\n",
    "print(formatted.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787144ef",
   "metadata": {},
   "source": [
    "### TUNING FOR RECALL OF CLASS 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "82d33e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Best parameters for Recall (classe 0): {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "✅ Best Recall (classe 0) from CV: 0.9988561533663789\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Split do dataset\n",
    "X_train, X_test, y_train, y_test = df1_raw[0]\n",
    "\n",
    "# Grid de parâmetros (exemplo, substitua pelo seu)\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Validação cruzada\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Scorer focado em recall da classe 0\n",
    "recall_class0 = make_scorer(recall_score, pos_label=0)\n",
    "\n",
    "# GridSearch\n",
    "grid_search_recall = GridSearchCV(\n",
    "    estimator=XGBClassifier(eval_metric='logloss', random_state=42),\n",
    "    param_grid=xgb_param_grid,\n",
    "    cv=cv,\n",
    "    scoring=recall_class0,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "grid_search_recall.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n📈 Best parameters for Recall (classe 0):\", grid_search_recall.best_params_)\n",
    "print(\"✅ Best Recall (classe 0) from CV:\", grid_search_recall.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "83e165ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Recall Tune: df1_raw            0.8717 ± 0.0042  0.8962 ± 0.0035   0.9403 ± 0.004  0.9177 ± 0.0027  0.9275 ± 0.0036\n",
      "\n",
      "📊 XGBoost Performance for class 0 (mean ± std):\n",
      "\n",
      "                accuracy      precision           recall              f1          roc_auc\n",
      "dataset                                                                                  \n",
      "df1_raw  0.8161 ± 0.0023  0.806 ± 0.002  0.9986 ± 0.0003  0.892 ± 0.0012  0.8905 ± 0.0033\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# Dataset\n",
    "datasets = {\n",
    "    'df1_raw': df1_raw,\n",
    "}\n",
    "\n",
    "xgb_results = []\n",
    "\n",
    "for name, splits in datasets.items():\n",
    "    \n",
    "    for i, (X_train, X_test, y_train, y_test) in enumerate(splits):\n",
    "\n",
    "        # Modelo com melhores parâmetros encontrados\n",
    "        model = XGBClassifier(\n",
    "            eval_metric='logloss',\n",
    "            random_state=42,\n",
    "            colsample_bytree=1.0,\n",
    "            learning_rate=0.01,\n",
    "            max_depth=3,\n",
    "            n_estimators=100,\n",
    "            subsample=1.0\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Métricas com foco na classe 0\n",
    "        metrics = {\n",
    "            'dataset': name,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred, pos_label=0),\n",
    "            'recall': recall_score(y_test, y_pred, pos_label=0),\n",
    "            'f1': f1_score(y_test, y_pred, pos_label=0),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba)  # Classe 1 como positiva\n",
    "        }\n",
    "\n",
    "        # Collect feature importances\n",
    "        feature_importances = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "        feature_importances.name = f\"{name}_split_{i}\"\n",
    "        xgb_results.append(metrics)\n",
    "        # Save importance separately\n",
    "        if 'feature_importance_aid' not in locals():\n",
    "            feature_importance_aid = []\n",
    "        feature_importance_aid.append(feature_importances)\n",
    "\n",
    "        xgb_results.append(metrics)\n",
    "\n",
    "# DataFrame\n",
    "xgb_results_df = pd.DataFrame(xgb_results)\n",
    "\n",
    "# Group by dataset and compute mean and std\n",
    "grouped = xgb_results_df.groupby('dataset').agg(['mean', 'std']).round(4)\n",
    "\n",
    "# Create a new DataFrame to store \"mean ± std\" strings\n",
    "formatted = pd.DataFrame(index=grouped.index)\n",
    "\n",
    "# Loop through metric columns\n",
    "for metric in grouped.columns.levels[0]:\n",
    "    mean_col = (metric, 'mean')\n",
    "    std_col = (metric, 'std')\n",
    "    \n",
    "    formatted[metric] = grouped[mean_col].astype(str) + ' ± ' + grouped[std_col].astype(str)\n",
    "\n",
    "# Save this model in a pickle file\n",
    "import pickle\n",
    "\n",
    "# Save model and feature names together\n",
    "model_package = {\n",
    "    'model': model,\n",
    "    'feature_names': model.feature_names_in_.tolist()\n",
    "}\n",
    "\n",
    "with open('xgb_model_recall_class_zero_tuned.pkl', 'wb') as f:\n",
    "    pickle.dump(model_package, f)\n",
    "\n",
    "\n",
    "# Print pretty table\n",
    "print(\"Before Recall Tune: df1_raw            0.8717 ± 0.0042  0.8962 ± 0.0035   0.9403 ± 0.004  0.9177 ± 0.0027  0.9275 ± 0.0036\")\n",
    "print(\"\\n📊 XGBoost Performance for class 0 (mean ± std):\\n\")\n",
    "print(formatted.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c707aac",
   "metadata": {},
   "source": [
    "### Fairness analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae41264",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c81497ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Top 15 Most Important Features (Average over folds):\n",
      "\n",
      "marital-status_Married-civ-spouse    0.301623\n",
      "relationship_Husband                 0.260074\n",
      "marital-status_Never-married         0.157799\n",
      "relationship_Wife                    0.067400\n",
      "education-num                        0.060890\n",
      "occupation_Prof-specialty            0.051147\n",
      "sex_Male                             0.045653\n",
      "capital-gain                         0.041618\n",
      "occupation_Exec-managerial           0.041020\n",
      "relationship_Not-in-family           0.025879\n",
      "age                                  0.023480\n",
      "capital-loss                         0.023059\n",
      "hours-per-week                       0.014954\n",
      "workclass_Self-emp-not-inc           0.007893\n",
      "relationship_Unmarried               0.004939\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# Combine all feature importances into a single DataFrame\n",
    "importances_df = pd.concat(feature_importance_bank, axis=1)\n",
    "\n",
    "# Average across all splits\n",
    "mean_importances = importances_df.mean(axis=1).sort_values(ascending=False)\n",
    "\n",
    "# Display top N important features\n",
    "top_n = 15\n",
    "print(f\"\\n Top {top_n} Most Important Features (Average over folds):\\n\")\n",
    "print(mean_importances.head(top_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "79588633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Top 15 Most Important Features (Average over folds):\n",
      "\n",
      "relationship_Husband                 0.316560\n",
      "marital-status_Married-civ-spouse    0.259632\n",
      "marital-status_Never-married         0.185853\n",
      "relationship_Wife                    0.057872\n",
      "education-num                        0.049626\n",
      "age                                  0.045231\n",
      "sex_Male                             0.036131\n",
      "capital-gain                         0.030840\n",
      "occupation_Exec-managerial           0.024030\n",
      "occupation_Other-service             0.023176\n",
      "relationship_Not-in-family           0.023043\n",
      "hours-per-week                       0.021638\n",
      "marital-status_Divorced              0.018954\n",
      "relationship_Own-child               0.018732\n",
      "occupation_Prof-specialty            0.017424\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# Combine all feature importances into a single DataFrame\n",
    "importances_df = pd.concat(feature_importance_irs, axis=1)\n",
    "\n",
    "# Average across all splits\n",
    "mean_importances = importances_df.mean(axis=1).sort_values(ascending=False)\n",
    "\n",
    "# Display top N important features\n",
    "top_n = 15\n",
    "print(f\"\\n Top {top_n} Most Important Features (Average over folds):\\n\")\n",
    "print(mean_importances.head(top_n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d28f9c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Top 15 Most Important Features (Average over folds):\n",
      "\n",
      "marital-status_Married-civ-spouse    0.301623\n",
      "relationship_Husband                 0.260074\n",
      "marital-status_Never-married         0.157799\n",
      "relationship_Wife                    0.067400\n",
      "education-num                        0.060890\n",
      "occupation_Prof-specialty            0.051147\n",
      "sex_Male                             0.045653\n",
      "capital-gain                         0.041618\n",
      "occupation_Exec-managerial           0.041020\n",
      "relationship_Not-in-family           0.025879\n",
      "age                                  0.023480\n",
      "capital-loss                         0.023059\n",
      "hours-per-week                       0.014954\n",
      "workclass_Self-emp-not-inc           0.007893\n",
      "relationship_Unmarried               0.004939\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# Combine all feature importances into a single DataFrame\n",
    "importances_df = pd.concat(feature_importance_aid, axis=1)\n",
    "\n",
    "# Average across all splits\n",
    "mean_importances = importances_df.mean(axis=1).sort_values(ascending=False)\n",
    "\n",
    "# Display top N important features\n",
    "top_n = 15\n",
    "print(f\"\\n Top {top_n} Most Important Features (Average over folds):\\n\")\n",
    "print(mean_importances.head(top_n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68921ab2",
   "metadata": {},
   "source": [
    "In alignment with the EU AI Act’s fairness and non-discrimination requirements for high-risk systems, we excluded features directly encoding protected characteristics such as race, sex, and nationality. Additionally, we assessed proxy variables like marital-status and relationship for indirect bias, and chose to exclude them based on their potential to encode demographic disparities.\n",
    "\n",
    "So to comply we will drop sex race native-country relashionship, marital-status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "78ea3d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features in df1_raw_fair:\n",
      "['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week', 'workclass_Federal-gov', 'workclass_Local-gov', 'workclass_Never-worked', 'workclass_Private', 'workclass_Self-emp-inc', 'workclass_Self-emp-not-inc', 'workclass_State-gov', 'workclass_Without-pay', 'marital-status_Divorced', 'marital-status_Married-AF-spouse', 'marital-status_Married-civ-spouse', 'marital-status_Married-spouse-absent', 'marital-status_Never-married', 'marital-status_Separated', 'marital-status_Widowed', 'occupation_Adm-clerical', 'occupation_Armed-Forces', 'occupation_Craft-repair', 'occupation_Exec-managerial', 'occupation_Farming-fishing', 'occupation_Handlers-cleaners', 'occupation_Machine-op-inspct', 'occupation_Other-service', 'occupation_Priv-house-serv', 'occupation_Prof-specialty', 'occupation_Protective-serv', 'occupation_Sales', 'occupation_Tech-support', 'occupation_Transport-moving', 'relationship_Husband', 'relationship_Not-in-family', 'relationship_Other-relative', 'relationship_Own-child', 'relationship_Unmarried', 'relationship_Wife', 'race_Amer-Indian-Eskimo', 'race_Asian-Pac-Islander', 'race_Black', 'race_Other', 'race_White', 'sex_Female', 'sex_Male', 'native-country_Cambodia', 'native-country_Canada', 'native-country_China', 'native-country_Columbia', 'native-country_Cuba', 'native-country_Dominican-Republic', 'native-country_Ecuador', 'native-country_El-Salvador', 'native-country_England', 'native-country_France', 'native-country_Germany', 'native-country_Greece', 'native-country_Guatemala', 'native-country_Haiti', 'native-country_Holand-Netherlands', 'native-country_Honduras', 'native-country_Hong', 'native-country_Hungary', 'native-country_India', 'native-country_Iran', 'native-country_Ireland', 'native-country_Italy', 'native-country_Jamaica', 'native-country_Japan', 'native-country_Laos', 'native-country_Mexico', 'native-country_Nicaragua', 'native-country_Outlying-US(Guam-USVI-etc)', 'native-country_Peru', 'native-country_Philippines', 'native-country_Poland', 'native-country_Portugal', 'native-country_Puerto-Rico', 'native-country_Scotland', 'native-country_South', 'native-country_Taiwan', 'native-country_Thailand', 'native-country_Trinadad&Tobago', 'native-country_United-States', 'native-country_Vietnam', 'native-country_Yugoslavia']\n",
      "Shape:  (39073, 89)\n",
      "\n",
      "Features in df1_UnderSampling_fair:\n",
      "['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week', 'workclass_Federal-gov', 'workclass_Local-gov', 'workclass_Never-worked', 'workclass_Private', 'workclass_Self-emp-inc', 'workclass_Self-emp-not-inc', 'workclass_State-gov', 'workclass_Without-pay', 'marital-status_Divorced', 'marital-status_Married-AF-spouse', 'marital-status_Married-civ-spouse', 'marital-status_Married-spouse-absent', 'marital-status_Never-married', 'marital-status_Separated', 'marital-status_Widowed', 'occupation_Adm-clerical', 'occupation_Armed-Forces', 'occupation_Craft-repair', 'occupation_Exec-managerial', 'occupation_Farming-fishing', 'occupation_Handlers-cleaners', 'occupation_Machine-op-inspct', 'occupation_Other-service', 'occupation_Priv-house-serv', 'occupation_Prof-specialty', 'occupation_Protective-serv', 'occupation_Sales', 'occupation_Tech-support', 'occupation_Transport-moving', 'relationship_Husband', 'relationship_Not-in-family', 'relationship_Other-relative', 'relationship_Own-child', 'relationship_Unmarried', 'relationship_Wife', 'race_Amer-Indian-Eskimo', 'race_Asian-Pac-Islander', 'race_Black', 'race_Other', 'race_White', 'sex_Female', 'sex_Male', 'native-country_Cambodia', 'native-country_Canada', 'native-country_China', 'native-country_Columbia', 'native-country_Cuba', 'native-country_Dominican-Republic', 'native-country_Ecuador', 'native-country_El-Salvador', 'native-country_England', 'native-country_France', 'native-country_Germany', 'native-country_Greece', 'native-country_Guatemala', 'native-country_Haiti', 'native-country_Holand-Netherlands', 'native-country_Honduras', 'native-country_Hong', 'native-country_Hungary', 'native-country_India', 'native-country_Iran', 'native-country_Ireland', 'native-country_Italy', 'native-country_Jamaica', 'native-country_Japan', 'native-country_Laos', 'native-country_Mexico', 'native-country_Nicaragua', 'native-country_Outlying-US(Guam-USVI-etc)', 'native-country_Peru', 'native-country_Philippines', 'native-country_Poland', 'native-country_Portugal', 'native-country_Puerto-Rico', 'native-country_Scotland', 'native-country_South', 'native-country_Taiwan', 'native-country_Thailand', 'native-country_Trinadad&Tobago', 'native-country_United-States', 'native-country_Vietnam', 'native-country_Yugoslavia']\n",
      "Shape:  (18698, 89)\n"
     ]
    }
   ],
   "source": [
    "df1_raw_fair = df1_raw.copy()\n",
    "df1_UnderSampling_fair = df1_UnderSampling.copy()\n",
    "#print the features for df1_raw_fair and df1_UnderSampling_fair\n",
    "print(\"\\nFeatures in df1_raw_fair:\")\n",
    "print(df1_raw_fair[0][0].columns.tolist())\n",
    "print(\"Shape: \", df1_raw_fair[0][0].shape)\n",
    "print(\"\\nFeatures in df1_UnderSampling_fair:\")\n",
    "print(df1_UnderSampling_fair[0][0].columns.tolist())\n",
    "print(\"Shape: \", df1_UnderSampling_fair[0][0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7f22758d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features in df1_raw_fair:\n",
      "['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week', 'workclass_Federal-gov', 'workclass_Local-gov', 'workclass_Never-worked', 'workclass_Private', 'workclass_Self-emp-inc', 'workclass_Self-emp-not-inc', 'workclass_State-gov', 'workclass_Without-pay', 'occupation_Adm-clerical', 'occupation_Armed-Forces', 'occupation_Craft-repair', 'occupation_Exec-managerial', 'occupation_Farming-fishing', 'occupation_Handlers-cleaners', 'occupation_Machine-op-inspct', 'occupation_Other-service', 'occupation_Priv-house-serv', 'occupation_Prof-specialty', 'occupation_Protective-serv', 'occupation_Sales', 'occupation_Tech-support', 'occupation_Transport-moving']\n",
      "Shape:  (39073, 28)\n",
      "\n",
      "Features in df1_UnderSampling_fair:\n",
      "['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week', 'workclass_Federal-gov', 'workclass_Local-gov', 'workclass_Never-worked', 'workclass_Private', 'workclass_Self-emp-inc', 'workclass_Self-emp-not-inc', 'workclass_State-gov', 'workclass_Without-pay', 'occupation_Adm-clerical', 'occupation_Armed-Forces', 'occupation_Craft-repair', 'occupation_Exec-managerial', 'occupation_Farming-fishing', 'occupation_Handlers-cleaners', 'occupation_Machine-op-inspct', 'occupation_Other-service', 'occupation_Priv-house-serv', 'occupation_Prof-specialty', 'occupation_Protective-serv', 'occupation_Sales', 'occupation_Tech-support', 'occupation_Transport-moving']\n",
      "Shape:  (18698, 28)\n"
     ]
    }
   ],
   "source": [
    "#drop collumns that start with sex race native-country relashionship, marital-status\n",
    "\n",
    "sensitive_prefixes = [\"sex\", \"race\", \"native-country\", \"relationship\", \"marital-status\"]\n",
    "\n",
    "columns_to_drop = [\n",
    "    col for col in df1_raw_fair[0][0].columns\n",
    "    if any(col.startswith(prefix) for prefix in sensitive_prefixes)\n",
    "]\n",
    "\n",
    "df1_raw_fair = [\n",
    "    (X_train.drop(columns=columns_to_drop), X_test.drop(columns=columns_to_drop), y_train, y_test)\n",
    "    for X_train, X_test, y_train, y_test in df1_raw_fair\n",
    "]\n",
    "\n",
    "df1_UnderSampling_fair = [\n",
    "    (X_train.drop(columns=columns_to_drop), X_test.drop(columns=columns_to_drop), y_train, y_test)\n",
    "    for X_train, X_test, y_train, y_test in df1_UnderSampling_fair\n",
    "]\n",
    "\n",
    "print(\"\\nFeatures in df1_raw_fair:\")\n",
    "print(df1_raw_fair[0][0].columns.tolist())\n",
    "print(\"Shape: \", df1_raw_fair[0][0].shape)\n",
    "print(\"\\nFeatures in df1_UnderSampling_fair:\")\n",
    "print(df1_UnderSampling_fair[0][0].columns.tolist())\n",
    "print(\"Shape: \", df1_UnderSampling_fair[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f714f3",
   "metadata": {},
   "source": [
    "### how will this afect model performance? What will be the new most important features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7c270684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Fairness: df1_raw  0.8161 ± 0.0023  0.9819 ± 0.0031  0.2361 ± 0.0099  0.3806 ± 0.0128  0.8905 ± 0.0033\n",
      "\n",
      "📊 XGBoost Performance (mean ± std):\n",
      "\n",
      "               accuracy        precision           recall              f1          roc_auc\n",
      "dataset                                                                                   \n",
      "df1_raw  0.809 ± 0.0025  0.9864 ± 0.0055  0.2046 ± 0.0111  0.3388 ± 0.015  0.8384 ± 0.0044\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# List of dataset split arrays (already defined)\n",
    "datasets = {\n",
    "    'df1_raw': df1_raw_fair,\n",
    "}\n",
    "\n",
    "# Initialize list to collect results\n",
    "xgb_results = []\n",
    "\n",
    "for name, splits in datasets.items():\n",
    "    \n",
    "    for i, (X_train, X_test, y_train, y_test) in enumerate(splits):\n",
    "\n",
    "        # Initialize XGBoost model\n",
    "        model = XGBClassifier(eval_metric='logloss', random_state=42, colsample_bytree=1.0, learning_rate=0.01, max_depth= 3, n_estimators= 100, subsample= 1.0)\n",
    "\n",
    "        # Train\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Metrics\n",
    "        metrics = {\n",
    "            'dataset': name,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "        }\n",
    "\n",
    "        # Collect feature importances\n",
    "        feature_importances = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "        feature_importances.name = f\"{name}_split_{i}\"\n",
    "        xgb_results.append(metrics)\n",
    "        # Save importance separately\n",
    "        if 'feature_importance_bank_fair' not in locals():\n",
    "            feature_importance_bank_fair = []\n",
    "        feature_importance_bank_fair.append(feature_importances)\n",
    "\n",
    "\n",
    "        xgb_results.append(metrics)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "xgb_results_df = pd.DataFrame(xgb_results)\n",
    "\n",
    "# Group by dataset and compute mean and std\n",
    "grouped = xgb_results_df.groupby('dataset').agg(['mean', 'std']).round(4)\n",
    "\n",
    "# Create a new DataFrame to store \"mean ± std\" strings\n",
    "formatted = pd.DataFrame(index=grouped.index)\n",
    "\n",
    "# Loop through metric columns\n",
    "for metric in grouped.columns.levels[0]:\n",
    "    mean_col = (metric, 'mean')\n",
    "    std_col = (metric, 'std')\n",
    "    \n",
    "    formatted[metric] = grouped[mean_col].astype(str) + ' ± ' + grouped[std_col].astype(str)\n",
    "\n",
    "# Print pretty table\n",
    "print(\"Before Fairness: df1_raw  0.8161 ± 0.0023  0.9819 ± 0.0031  0.2361 ± 0.0099  0.3806 ± 0.0128  0.8905 ± 0.0033\")\n",
    "print(\"\\n📊 XGBoost Performance (mean ± std):\\n\")\n",
    "print(formatted.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ffacb583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Top 15 Most Important Features (Average over folds):\n",
      "\n",
      "education-num                   0.375755\n",
      "capital-gain                    0.175930\n",
      "age                             0.158893\n",
      "hours-per-week                  0.137342\n",
      "capital-loss                    0.086397\n",
      "occupation_Exec-managerial      0.064707\n",
      "workclass_Self-emp-not-inc      0.000976\n",
      "occupation_Tech-support         0.000000\n",
      "occupation_Sales                0.000000\n",
      "occupation_Protective-serv      0.000000\n",
      "occupation_Prof-specialty       0.000000\n",
      "occupation_Priv-house-serv      0.000000\n",
      "occupation_Other-service        0.000000\n",
      "occupation_Machine-op-inspct    0.000000\n",
      "occupation_Handlers-cleaners    0.000000\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# Combine all feature importances into a single DataFrame\n",
    "importances_df = pd.concat(feature_importance_bank_fair, axis=1)\n",
    "\n",
    "# Average across all splits\n",
    "mean_importances = importances_df.mean(axis=1).sort_values(ascending=False)\n",
    "\n",
    "# Display top N important features\n",
    "top_n = 15\n",
    "print(f\"\\n Top {top_n} Most Important Features (Average over folds):\\n\")\n",
    "print(mean_importances.head(top_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "01b7936d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Fairness: df1_UnderSampling  0.7789 ± 0.0066  0.5228 ± 0.0089  0.88 ± 0.0103  0.6558 ± 0.006  0.902 ± 0.0031\n",
      "\n",
      "📊 XGBoost Performance (mean ± std):\n",
      "\n",
      "                          accuracy       precision           recall               f1          roc_auc\n",
      "dataset                                                                                              \n",
      "df1_UnderSampling  0.7512 ± 0.0063  0.4878 ± 0.008  0.7709 ± 0.0226  0.5973 ± 0.0046  0.8524 ± 0.0039\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# List of dataset split arrays (already defined)\n",
    "datasets = {\n",
    "    'df1_UnderSampling': df1_UnderSampling_fair,\n",
    "}\n",
    "\n",
    "# Initialize list to collect results\n",
    "xgb_results = []\n",
    "\n",
    "for name, splits in datasets.items():\n",
    "    \n",
    "    for i, (X_train, X_test, y_train, y_test) in enumerate(splits):\n",
    "\n",
    "        # Initialize XGBoost model\n",
    "        model = XGBClassifier(eval_metric='logloss', random_state=42, colsample_bytree=0.8, learning_rate=0.01, max_depth= 3, n_estimators= 200, subsample= 1.0)\n",
    "\n",
    "        # Train\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Metrics\n",
    "        metrics = {\n",
    "            'dataset': name,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "        }\n",
    "\n",
    "        # Collect feature importances\n",
    "        feature_importances = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "        feature_importances.name = f\"{name}_split_{i}\"\n",
    "        xgb_results.append(metrics)\n",
    "        # Save importance separately\n",
    "        if 'feature_importance_irs_fair' not in locals():\n",
    "            feature_importance_irs_fair = []\n",
    "        feature_importance_irs_fair.append(feature_importances)\n",
    "\n",
    "        xgb_results.append(metrics)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "xgb_results_df = pd.DataFrame(xgb_results)\n",
    "\n",
    "# Group by dataset and compute mean and std\n",
    "grouped = xgb_results_df.groupby('dataset').agg(['mean', 'std']).round(4)\n",
    "\n",
    "# Create a new DataFrame to store \"mean ± std\" strings\n",
    "formatted = pd.DataFrame(index=grouped.index)\n",
    "\n",
    "# Loop through metric columns\n",
    "for metric in grouped.columns.levels[0]:\n",
    "    mean_col = (metric, 'mean')\n",
    "    std_col = (metric, 'std')\n",
    "    \n",
    "    formatted[metric] = grouped[mean_col].astype(str) + ' ± ' + grouped[std_col].astype(str)\n",
    "\n",
    "# Print pretty table\n",
    "print(\"Before Fairness: df1_UnderSampling  0.7789 ± 0.0066  0.5228 ± 0.0089  0.88 ± 0.0103  0.6558 ± 0.006  0.902 ± 0.0031\")\n",
    "print(\"\\n📊 XGBoost Performance (mean ± std):\\n\")\n",
    "print(formatted.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "57f5512d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Top 15 Most Important Features (Average over folds):\n",
      "\n",
      "age                            0.246976\n",
      "education-num                  0.223848\n",
      "occupation_Other-service       0.105797\n",
      "hours-per-week                 0.098340\n",
      "occupation_Prof-specialty      0.082600\n",
      "capital-gain                   0.077317\n",
      "occupation_Exec-managerial     0.062415\n",
      "capital-loss                   0.049985\n",
      "occupation_Adm-clerical        0.029075\n",
      "occupation_Farming-fishing     0.010447\n",
      "workclass_Self-emp-inc         0.007904\n",
      "workclass_Self-emp-not-inc     0.003609\n",
      "fnlwgt                         0.000578\n",
      "occupation_Transport-moving    0.000394\n",
      "occupation_Protective-serv     0.000303\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# Combine all feature importances into a single DataFrame\n",
    "importances_df = pd.concat(feature_importance_irs_fair, axis=1)\n",
    "\n",
    "# Average across all splits\n",
    "mean_importances = importances_df.mean(axis=1).sort_values(ascending=False)\n",
    "\n",
    "# Display top N important features\n",
    "top_n = 15\n",
    "print(f\"\\n Top {top_n} Most Important Features (Average over folds):\\n\")\n",
    "print(mean_importances.head(top_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4b5283c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Fairness: df1_raw  0.8161 ± 0.0023  0.806 ± 0.002  0.9986 ± 0.0003  0.892 ± 0.0012  0.8905 ± 0.0033\n",
      "\n",
      "📊 XGBoost Performance for class 0 (mean ± std):\n",
      "\n",
      "                    accuracy        precision           recall               f1          roc_auc\n",
      "dataset                                                                                         \n",
      "df1_raw_fair  0.809 ± 0.0025  0.7997 ± 0.0022  0.9991 ± 0.0004  0.8883 ± 0.0013  0.8384 ± 0.0044\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# Dataset\n",
    "datasets = {\n",
    "    'df1_raw_fair': df1_raw_fair,\n",
    "}\n",
    "\n",
    "xgb_results = []\n",
    "\n",
    "for name, splits in datasets.items():\n",
    "    \n",
    "    for i, (X_train, X_test, y_train, y_test) in enumerate(splits):\n",
    "\n",
    "        # Modelo com melhores parâmetros encontrados\n",
    "        model = XGBClassifier(\n",
    "            eval_metric='logloss',\n",
    "            random_state=42,\n",
    "            colsample_bytree=1.0,\n",
    "            learning_rate=0.01,\n",
    "            max_depth=3,\n",
    "            n_estimators=100,\n",
    "            subsample=1.0\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Métricas com foco na classe 0\n",
    "        metrics = {\n",
    "            'dataset': name,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred, pos_label=0),\n",
    "            'recall': recall_score(y_test, y_pred, pos_label=0),\n",
    "            'f1': f1_score(y_test, y_pred, pos_label=0),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba)  # Classe 1 como positiva\n",
    "        }\n",
    "\n",
    "        # Collect feature importances\n",
    "        feature_importances = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "        feature_importances.name = f\"{name}_split_{i}\"\n",
    "        xgb_results.append(metrics)\n",
    "        # Save importance separately\n",
    "        if 'feature_importance_aid_fair' not in locals():\n",
    "            feature_importance_aid_fair = []\n",
    "        feature_importance_aid_fair.append(feature_importances)\n",
    "\n",
    "        xgb_results.append(metrics)\n",
    "\n",
    "# DataFrame\n",
    "xgb_results_df = pd.DataFrame(xgb_results)\n",
    "\n",
    "# Group by dataset and compute mean and std\n",
    "grouped = xgb_results_df.groupby('dataset').agg(['mean', 'std']).round(4)\n",
    "\n",
    "# Create a new DataFrame to store \"mean ± std\" strings\n",
    "formatted = pd.DataFrame(index=grouped.index)\n",
    "\n",
    "# Loop through metric columns\n",
    "for metric in grouped.columns.levels[0]:\n",
    "    mean_col = (metric, 'mean')\n",
    "    std_col = (metric, 'std')\n",
    "    \n",
    "    formatted[metric] = grouped[mean_col].astype(str) + ' ± ' + grouped[std_col].astype(str)\n",
    "\n",
    "# Print pretty table\n",
    "print(\"Before Fairness: df1_raw  0.8161 ± 0.0023  0.806 ± 0.002  0.9986 ± 0.0003  0.892 ± 0.0012  0.8905 ± 0.0033\")\n",
    "print(\"\\n📊 XGBoost Performance for class 0 (mean ± std):\\n\")\n",
    "print(formatted.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f4e95b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Top 15 Most Important Features (Average over folds):\n",
      "\n",
      "education-num                   0.375755\n",
      "capital-gain                    0.175930\n",
      "age                             0.158893\n",
      "hours-per-week                  0.137342\n",
      "capital-loss                    0.086397\n",
      "occupation_Exec-managerial      0.064707\n",
      "workclass_Self-emp-not-inc      0.000976\n",
      "occupation_Tech-support         0.000000\n",
      "occupation_Sales                0.000000\n",
      "occupation_Protective-serv      0.000000\n",
      "occupation_Prof-specialty       0.000000\n",
      "occupation_Priv-house-serv      0.000000\n",
      "occupation_Other-service        0.000000\n",
      "occupation_Machine-op-inspct    0.000000\n",
      "occupation_Handlers-cleaners    0.000000\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# Combine all feature importances into a single DataFrame\n",
    "importances_df = pd.concat(feature_importance_aid_fair, axis=1)\n",
    "\n",
    "# Average across all splits\n",
    "mean_importances = importances_df.mean(axis=1).sort_values(ascending=False)\n",
    "\n",
    "# Display top N important features\n",
    "top_n = 15\n",
    "print(f\"\\n Top {top_n} Most Important Features (Average over folds):\\n\")\n",
    "print(mean_importances.head(top_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12b1fd7",
   "metadata": {},
   "source": [
    "what is roc_auc seems to be the main metric to lose performance when i remove the features to ensure fairness\n",
    "\n",
    "\n",
    "Dito pelo ChatGPT:\n",
    "Great question — and your observation is spot on. When removing features for fairness, particularly sensitive attributes like sex, race, native-country, and related interactions, ROC AUC often takes the biggest hit among your metrics.\n",
    "\n",
    "Here's why that happens and what it means:\n",
    "\n",
    "🔍 What is ROC AUC?\n",
    "ROC AUC (Receiver Operating Characteristic Area Under the Curve) measures how well a model distinguishes between the two classes across all thresholds. It's a measure of ranking quality — the model's ability to assign higher scores to positive cases than negative ones.\n",
    "\n",
    "AUC = 1.0 → perfect separation\n",
    "\n",
    "AUC = 0.5 → random guessing\n",
    "\n",
    "🚨 Why does ROC AUC drop when you remove sensitive features?\n",
    "Because:\n",
    "\n",
    "Sensitive features often correlate strongly with the target (income) in datasets like the UCI Adult dataset.\n",
    "\n",
    "E.g., sex and race are often proxies for structural inequalities in the data, and models pick up on them.\n",
    "\n",
    "These features improve discrimination power — even if that comes at the cost of fairness.\n",
    "\n",
    "ROC AUC is threshold-independent, so when you remove predictive-but-biased features, your model loses the ability to rank examples correctly across the score spectrum.\n",
    "\n",
    "In contrast, metrics like accuracy or F1 may be less sensitive because they depend on a fixed threshold (e.g., 0.5) and not overall score quality.\n",
    "\n",
    "📊 Example:\n",
    "Say a model uses sex_Male = 1 as a strong signal for predicting >50K. Removing it:\n",
    "\n",
    "Makes the model rely more on less informative features.\n",
    "\n",
    "Hurts its ability to rank individuals appropriately, even if the fixed-threshold prediction still hits similar accuracy.\n",
    "\n",
    "⚖️ What does this mean for fairness?\n",
    "This is expected, and it’s part of the trade-off:\n",
    "\n",
    "You are choosing fairer decision-making (no discrimination based on protected attributes),\n",
    "\n",
    "At the cost of slight performance degradation, particularly in ranking-based metrics like ROC AUC.\n",
    "\n",
    "In a technical report, you could write something like:\n",
    "\n",
    "\"We observed a notable decrease in ROC AUC after removing sensitive features such as race, sex, and native-country. This suggests that while these attributes were predictive, their removal reduced the model’s ability to rank positive and negative instances effectively — a typical trade-off when prioritizing algorithmic fairness over raw discriminative power.\"\n",
    "\n",
    "Let me know if you want to quantify or visualize this trade-off for your report!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341c570f",
   "metadata": {},
   "source": [
    "### How does the model predict us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab47cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Feature list (used by all models):\n",
      "age\n",
      "fnlwgt\n",
      "education-num\n",
      "capital-gain\n",
      "capital-loss\n",
      "hours-per-week\n",
      "workclass_Federal-gov\n",
      "workclass_Local-gov\n",
      "workclass_Never-worked\n",
      "workclass_Private\n",
      "workclass_Self-emp-inc\n",
      "workclass_Self-emp-not-inc\n",
      "workclass_State-gov\n",
      "workclass_Without-pay\n",
      "marital-status_Divorced\n",
      "marital-status_Married-AF-spouse\n",
      "marital-status_Married-civ-spouse\n",
      "marital-status_Married-spouse-absent\n",
      "marital-status_Never-married\n",
      "marital-status_Separated\n",
      "marital-status_Widowed\n",
      "occupation_Adm-clerical\n",
      "occupation_Armed-Forces\n",
      "occupation_Craft-repair\n",
      "occupation_Exec-managerial\n",
      "occupation_Farming-fishing\n",
      "occupation_Handlers-cleaners\n",
      "occupation_Machine-op-inspct\n",
      "occupation_Other-service\n",
      "occupation_Priv-house-serv\n",
      "occupation_Prof-specialty\n",
      "occupation_Protective-serv\n",
      "occupation_Sales\n",
      "occupation_Tech-support\n",
      "occupation_Transport-moving\n",
      "relationship_Husband\n",
      "relationship_Not-in-family\n",
      "relationship_Other-relative\n",
      "relationship_Own-child\n",
      "relationship_Unmarried\n",
      "relationship_Wife\n",
      "race_Amer-Indian-Eskimo\n",
      "race_Asian-Pac-Islander\n",
      "race_Black\n",
      "race_Other\n",
      "race_White\n",
      "sex_Female\n",
      "sex_Male\n",
      "native-country_Cambodia\n",
      "native-country_Canada\n",
      "native-country_China\n",
      "native-country_Columbia\n",
      "native-country_Cuba\n",
      "native-country_Dominican-Republic\n",
      "native-country_Ecuador\n",
      "native-country_El-Salvador\n",
      "native-country_England\n",
      "native-country_France\n",
      "native-country_Germany\n",
      "native-country_Greece\n",
      "native-country_Guatemala\n",
      "native-country_Haiti\n",
      "native-country_Holand-Netherlands\n",
      "native-country_Honduras\n",
      "native-country_Hong\n",
      "native-country_Hungary\n",
      "native-country_India\n",
      "native-country_Iran\n",
      "native-country_Ireland\n",
      "native-country_Italy\n",
      "native-country_Jamaica\n",
      "native-country_Japan\n",
      "native-country_Laos\n",
      "native-country_Mexico\n",
      "native-country_Nicaragua\n",
      "native-country_Outlying-US(Guam-USVI-etc)\n",
      "native-country_Peru\n",
      "native-country_Philippines\n",
      "native-country_Poland\n",
      "native-country_Portugal\n",
      "native-country_Puerto-Rico\n",
      "native-country_Scotland\n",
      "native-country_South\n",
      "native-country_Taiwan\n",
      "native-country_Thailand\n",
      "native-country_Trinadad&Tobago\n",
      "native-country_United-States\n",
      "native-country_Vietnam\n",
      "native-country_Yugoslavia\n",
      "\n",
      "Total features: 89\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load all three models with feature metadata\n",
    "with open('xgb_model_precision_tuned.pkl', 'rb') as f:\n",
    "    precision_package = pickle.load(f)\n",
    "\n",
    "with open('xgb_model_recall_tuned.pkl', 'rb') as f:\n",
    "    recall_package = pickle.load(f)\n",
    "\n",
    "with open('xgb_model_recall_class_zero_tuned.pkl', 'rb') as f:\n",
    "    recall_zero_package = pickle.load(f)\n",
    "\n",
    "# Extract models and feature names\n",
    "model_precision = precision_package['model']\n",
    "model_recall = recall_package['model']\n",
    "model_recall_zero = recall_zero_package['model']\n",
    "\n",
    "features_precision = precision_package['feature_names']\n",
    "features_recall = recall_package['feature_names']\n",
    "features_recall_zero = recall_zero_package['feature_names']\n",
    "\n",
    "# ✅ Ensure all models expect the same features\n",
    "assert features_precision == features_recall == features_recall_zero, \"Models do not share the same feature list!\"\n",
    "\n",
    "\n",
    "# Print the common feature list\n",
    "print(\"\\n✅ Feature list (used by all models):\")\n",
    "for f in features_precision:\n",
    "    print(f)\n",
    "print(f\"\\nTotal features: {len(features_precision)}\")\n",
    "# Initialize input dictionaries with placeholder values\n",
    "hugo = {feature: 0 for feature in features_precision}\n",
    "pedro = {feature: 0 for feature in features_precision}\n",
    "\n",
    "hugo['age'] = 20\n",
    "pedro['age'] = 24\n",
    "hugo['fnlwgt'] = 189660\n",
    "pedro['fnlwgt'] = 189660\n",
    "hugo['education-num'] = 13\n",
    "pedro['education-num'] = 13\n",
    "hugo['workclass_Never-worked'] = 1\n",
    "pedro['workclass_Never-worked'] = 1\n",
    "hugo['marital-status_Never-married'] = 1\n",
    "pedro['marital-status_Never-married'] = 1\n",
    "hugo['occupation_Prof-specialty'] = 1 #filled because is the mode\n",
    "pedro['occupation_Prof-specialty'] = 1 #filled because is the mode\n",
    "hugo['relationship_Not-in-family'] = 1\n",
    "pedro['relationship_Not-in-family'] = 1\n",
    "hugo['race_White'] = 1\n",
    "pedro['race_Black'] = 1\n",
    "hugo['sex_Male'] = 1\n",
    "pedro['sex_Male'] = 1\n",
    "hugo['native-country_Portugal'] = 1\n",
    "pedro['native-country_Portugal'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ee4f7573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Predictions for Hugo:\n",
      "Precision-Optimized: The model predicted that Hugo WOULD NOT BE ACCEPTED for a high-risk loan (confidence: 85.6%).\n",
      "Recall-Optimized: The model predicted that the IRS is NOT coming after Hugo (confidence: 85.5%).\n",
      "Recall Class 0-Optimized: The model predicted that Hugo NEEDS social aid (confidence: 85.6%).\n",
      "\n",
      "🔎 Predictions for Pedro:\n",
      "Precision-Optimized: The model predicted that Pedro WOULD NOT BE ACCEPTED for a high-risk loan (confidence: 85.6%).\n",
      "Recall-Optimized: The model predicted that the IRS is NOT coming after Pedro (confidence: 85.2%).\n",
      "Recall Class 0-Optimized: The model predicted that Pedro NEEDS social aid (confidence: 85.6%).\n"
     ]
    }
   ],
   "source": [
    "# Convert to DataFrame\n",
    "hugo_df = pd.DataFrame([hugo])\n",
    "pedro_df = pd.DataFrame([pedro])\n",
    "\n",
    "def interpret_prediction(name, pred_class, prob, context):\n",
    "    prob_pct = prob * 100\n",
    "    if context == \"loan\":\n",
    "        if pred_class == 1:\n",
    "            return f\"The model predicted that {name} WOULD BE ACCEPTED for a high-risk loan (confidence: {prob_pct:.1f}%).\"\n",
    "        else:\n",
    "            return f\"The model predicted that {name} WOULD NOT BE ACCEPTED for a high-risk loan (confidence: {100 - prob_pct:.1f}%).\"\n",
    "    elif context == \"irs\":\n",
    "        if pred_class == 1:\n",
    "            return f\"The model predicted that the IRS IS COMING after {name} (confidence: {prob_pct:.1f}%).\"\n",
    "        else:\n",
    "            return f\"The model predicted that the IRS is NOT coming after {name} (confidence: {100 - prob_pct:.1f}%).\"\n",
    "    elif context == \"social_aid\":\n",
    "        if pred_class == 0:\n",
    "            return f\"The model predicted that {name} NEEDS social aid (confidence: {100 -prob_pct:.1f}%).\"\n",
    "        else:\n",
    "            return f\"The model predicted that {name} DOES NOT NEED social aid (confidence: {prob_pct:.1f}%).\"\n",
    "    else:\n",
    "        return f\"{name}'s prediction: Class={pred_class}, Probability={prob_pct:.1f}%\"\n",
    "\n",
    "def show_predictions(name, df):\n",
    "    print(f\"\\n🔎 Predictions for {name}:\")\n",
    "    \n",
    "    model_contexts = [\n",
    "        (\"Precision-Optimized\", model_precision, \"loan\"),\n",
    "        (\"Recall-Optimized\", model_recall, \"irs\"),\n",
    "        (\"Recall Class 0-Optimized\", model_recall_zero, \"social_aid\"),\n",
    "    ]\n",
    "    \n",
    "    for label, model, context in model_contexts:\n",
    "        pred_class = model.predict(df)[0]\n",
    "        pred_proba = model.predict_proba(df)[0][1]  # Probability of class 1\n",
    "        message = interpret_prediction(name, pred_class, pred_proba, context)\n",
    "        print(f\"{label}: {message}\")\n",
    "\n",
    "# Run predictions\n",
    "show_predictions(\"Hugo\", hugo_df)\n",
    "show_predictions(\"Pedro\", pedro_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
