{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d410884",
   "metadata": {},
   "source": [
    "# Dataset Context and Exploration\n",
    "\n",
    "**age**  \n",
    "- Demographic: Age  \n",
    "- Description: How old is this sample of people  \n",
    "- Data Type: Integer  \n",
    "- Missing Values: No  \n",
    "\n",
    "**workclass**  \n",
    "- Demographic: Income  \n",
    "- Description: Employment classification  \n",
    "- Possible values: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked  \n",
    "- Data Type: Categorical  \n",
    "- Missing Values: Yes  \n",
    "\n",
    "**fnlwgt**  \n",
    "- Demographic:  \n",
    "- Description: Final weight representing the number of people the census believes the entry represents  \n",
    "- Data Type: Integer  \n",
    "- Missing Values: No  \n",
    "\n",
    "**education**  \n",
    "- Demographic: Education Level  \n",
    "- Description: Highest level of education achieved  \n",
    "- Possible values: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool  \n",
    "- Data Type: Categorical  \n",
    "- Missing Values: No  \n",
    "\n",
    "**education-num**  \n",
    "- Demographic: Education Level  \n",
    "- Description: Numeric representation of education level  \n",
    "- Data Type: Integer  \n",
    "- Missing Values: No  \n",
    "\n",
    "**marital-status**  \n",
    "- Demographic: Other  \n",
    "- Description: Marital status of the individual  \n",
    "- Possible values: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse  \n",
    "- Data Type: Categorical  \n",
    "- Missing Values: No  \n",
    "\n",
    "**occupation**  \n",
    "- Demographic: Other  \n",
    "- Description: Type of job  \n",
    "- Possible values: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces  \n",
    "- Data Type: Categorical  \n",
    "- Missing Values: Yes  \n",
    "\n",
    "**relationship**  \n",
    "- Demographic: Other  \n",
    "- Description: Relationship within household  \n",
    "- Possible values: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried  \n",
    "- Data Type: Categorical  \n",
    "- Missing Values: No  \n",
    "\n",
    "**race**  \n",
    "- Demographic: Race  \n",
    "- Description: Race of the individual  \n",
    "- Possible values: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black  \n",
    "- Data Type: Categorical  \n",
    "- Missing Values: No  \n",
    "\n",
    "**sex**  \n",
    "- Demographic: Sex  \n",
    "- Description: Gender of the individual  \n",
    "- Possible values: Female, Male  \n",
    "- Data Type: Binary  \n",
    "- Missing Values: No  \n",
    "\n",
    "**capital-gain**  \n",
    "- Demographic:  \n",
    "- Description: Income from capital gains  \n",
    "- Data Type: Integer  \n",
    "- Missing Values: No  \n",
    "\n",
    "**capital-loss**  \n",
    "- Demographic:  \n",
    "- Description: Capital losses  \n",
    "- Data Type: Integer  \n",
    "- Missing Values: No  \n",
    "\n",
    "**hours-per-week**  \n",
    "- Demographic:  \n",
    "- Description: Number of hours worked per week  \n",
    "- Data Type: Integer  \n",
    "- Missing Values: No  \n",
    "\n",
    "**native-country**  \n",
    "- Demographic: Other  \n",
    "- Description: Country of origin  \n",
    "- Possible values: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands  \n",
    "- Data Type: Categorical  \n",
    "- Missing Values: Yes  \n",
    "\n",
    "**income**  \n",
    "- Demographic: Income  \n",
    "- Description: Income class  \n",
    "- Possible values: >50K, <=50K  \n",
    "- Data Type: Binary  \n",
    "- Missing Values: No  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfb6049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe9ed00",
   "metadata": {},
   "source": [
    "# Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f607c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                  0\n",
      "workclass         2799\n",
      "fnlwgt               0\n",
      "education            0\n",
      "education-num        0\n",
      "marital-status       0\n",
      "occupation        2809\n",
      "relationship         0\n",
      "race                 0\n",
      "sex                  0\n",
      "capital-gain         0\n",
      "capital-loss         0\n",
      "hours-per-week       0\n",
      "native-country     857\n",
      "income               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('base_data/adultdataset.csv', na_values='NaN', skipinitialspace=True)\n",
    "\n",
    "#Identify and count the number of missing values in each column, the missing values are repesented by \"NaN\"\n",
    "missing_values = df.isna().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca1cdf5",
   "metadata": {},
   "source": [
    "# Total fnlwgt\n",
    "\n",
    "The fnlwgt represent the weight a certain row has over the whole dataset, this works like a percentage.\n",
    "From this point forward if we discribe something as weighted it means that it weighted to the total fnlwgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a987500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9263575662\n"
     ]
    }
   ],
   "source": [
    "total_fnlwgt = df['fnlwgt'].sum()\n",
    "print(total_fnlwgt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57702e4b",
   "metadata": {},
   "source": [
    "# Weighted Age Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "376abf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'17-26': 0.23254280804728858, '27-36': 0.27141756733459493, '37-46': 0.23722836582580933, '47-56': 0.15117445207951713, '57-66': 0.07870687525021912, '67-76': 0.02373170685071477, '77-86': 0.004034587978086513, '87-99': 0.001163636633769635}\n"
     ]
    }
   ],
   "source": [
    "age_bins = [(17, 26), (27, 36), (37, 46), (47, 56), (57, 66), (67, 76), (77, 86), (87, 99)]\n",
    "\n",
    "age_distribution = (df.groupby('age')['fnlwgt'].sum() / total_fnlwgt).to_dict()\n",
    "\n",
    "# Aggregate into custom age ranges\n",
    "age_ranges = {}\n",
    "for start, end in age_bins:\n",
    "    label = f'{start}-{end}'\n",
    "    age_ranges[label] = sum(age_distribution.get(age, 0) for age in range(start, end + 1))\n",
    "\n",
    "print(age_ranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cf6cee",
   "metadata": {},
   "source": [
    "# Weighted Education Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bfcbf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10th': 0.029468504815025498, '11th': 0.03816294386736559, '12th': 0.01400998833877717, '1st-4th': 0.0062664863027056044, '5th-6th': 0.012629909796056028, '7th-8th': 0.019338862285637366, '9th': 0.016240940376528228, 'Assoc-acdm': 0.03347668679083759, 'Assoc-voc': 0.0399183750953639, 'Bachelors': 0.163176883759985, 'Doctorate': 0.011804269538010278, 'HS-grad': 0.32139634430936437, 'Masters': 0.0520525309657691, 'Preschool': 0.0021408804465594703, 'Prof-school': 0.016798332164364632, 'Some-college': 0.22311806114765018}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "education_distribution = (df.groupby('education')['fnlwgt'].sum() / total_fnlwgt).to_dict()\n",
    "print(education_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7d20fb",
   "metadata": {},
   "source": [
    "# Weighted Race Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bed862e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Amer-Indian-Eskimo': 0.00609399275827926, 'Asian-Pac-Islander': 0.026202793700461278, 'Black': 0.11656649596219383, 'Other': 0.008577742860778288, 'White': 0.8425589747182873}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "race_distribution = (df.groupby('race')['fnlwgt'].sum() / total_fnlwgt).to_dict()\n",
    "print(race_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bb02c1",
   "metadata": {},
   "source": [
    "# Weighted Native Country Distribuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e324f64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cambodia': 0.0006054133095717787, 'Canada': 0.0035612337183499113, 'China': 0.002275493585750992, 'Columbia': 0.001998964619672796, 'Cuba': 0.003584282917470276, 'Dominican-Republic': 0.002264667852399304, 'Ecuador': 0.000867478746135166, 'El-Salvador': 0.004194289701695103, 'England': 0.0025167153430435273, 'France': 0.0007650541495625774, 'Germany': 0.004291807337752816, 'Greece': 0.000795958306925307, 'Guatemala': 0.0024352646130521775, 'Haiti': 0.0017626972128033124, 'Holand-Netherlands': 3.009852892374422e-06, 'Honduras': 0.0005169305217253592, 'Hong': 0.0006895137723332388, 'Hungary': 0.00040688543360871404, 'India': 0.0026994449996861265, 'Iran': 0.0012345983254516651, 'Ireland': 0.000583518308397231, 'Italy': 0.0020298072457196714, 'Jamaica': 0.002418630971181892, 'Japan': 0.0019346626674100781, 'Laos': 0.0005085181113512882, 'Mexico': 0.02920982867446892, 'Nicaragua': 0.0015055085108452585, 'Outlying-US(Guam-USVI-etc)': 0.00046019119997985937, 'Peru': 0.0013488914492551592, 'Philippines': 0.005207780964956726, 'Poland': 0.0017265999203321452, 'Portugal': 0.0010919792064197425, 'Puerto-Rico': 0.004064267230465031, 'Scotland': 0.0003546663966352996, 'South': 0.0020754486929779234, 'Taiwan': 0.0012956487254953454, 'Thailand': 0.0005933743298009886, 'Trinadad&Tobago': 0.0006071571286530288, 'United-States': 0.8854902228141374, 'Vietnam': 0.0015862030533496602, 'Yugoslavia': 0.0005276729179264515}\n"
     ]
    }
   ],
   "source": [
    "ncountry_distribution = (df.groupby('native-country')['fnlwgt'].sum() / total_fnlwgt).to_dict()\n",
    "print(ncountry_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6b6cb8",
   "metadata": {},
   "source": [
    "# Weighted Workclass Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5416f357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Federal-gov': 0.028380069488549937, 'Local-gov': 0.06437528452930555, 'Never-worked': 0.00023212775265828018, 'Private': 0.7051966278850047, 'Self-emp-inc': 0.03275067868712141, 'Self-emp-not-inc': 0.07319917748192728, 'State-gov': 0.03890616400732109, 'Without-pay': 0.00038062581109622506}\n"
     ]
    }
   ],
   "source": [
    "workclass_distribution = (df.groupby('workclass')['fnlwgt'].sum() / total_fnlwgt).to_dict()\n",
    "print(workclass_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05717e4",
   "metadata": {},
   "source": [
    "# Weighted Income Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc469a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<=50K': 0.7622240390354357, '>50K': 0.2377759609645643}\n"
     ]
    }
   ],
   "source": [
    "income_distribution = (df.groupby('income')['fnlwgt'].sum() / total_fnlwgt).to_dict()\n",
    "print(income_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163346c7",
   "metadata": {},
   "source": [
    "# Weighted Sex Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91d60d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Female': 0.32424719304894256, 'Male': 0.6757528069510574}\n"
     ]
    }
   ],
   "source": [
    "sex_distribution = (df.groupby('sex')['fnlwgt'].sum() / total_fnlwgt).to_dict()\n",
    "print(sex_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be109aee",
   "metadata": {},
   "source": [
    "# Weighted Relationship Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b0598e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Husband': 0.39841025891757864, 'Not-in-family': 0.25856131437726904, 'Other-relative': 0.03308744508422627, 'Own-child': 0.15858494253209673, 'Unmarried': 0.10587409719372355, 'Wife': 0.04548194189510577}\n"
     ]
    }
   ],
   "source": [
    "relationship_distribution = (df.groupby('relationship')['fnlwgt'].sum() / total_fnlwgt).to_dict()\n",
    "print(relationship_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e18149",
   "metadata": {},
   "source": [
    "# Weighted Average Hours per Weak Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3600a96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.0005355909187801483, 2: 0.001110879143807656, 3: 0.0011775734768106653, 4: 0.001502640719727734, 5: 0.001782997041601753, 6: 0.0017477536310751623, 7: 0.0008876964252294569, 8: 0.004306831773788993, 9: 0.0005425860578457053, 10: 0.00822093884464027, 11: 0.00029105478255659766, 12: 0.00464480234954009, 13: 0.0005934932903449739, 14: 0.001068289757765461, 15: 0.012683411491090814, 16: 0.005952873923857416, 17: 0.0009196336610004795, 18: 0.002499484415610746, 19: 0.00037384246929681955, 20: 0.03775265175785208, 21: 0.0009668757860683624, 22: 0.0012101099412383685, 23: 0.000869856013920686, 24: 0.007465276748769972, 25: 0.020016737139702544, 26: 0.0008324194977574308, 27: 0.0008371259957222337, 28: 0.0029465861775135357, 29: 0.0003484678182358651, 30: 0.035563565627408374, 31: 0.00024778768844258833, 32: 0.008798474473991942, 33: 0.0013127072572940571, 34: 0.0009296414596500113, 35: 0.039750254268501276, 36: 0.007376472162936601, 37: 0.004874414119077986, 38: 0.014070435839875153, 39: 0.0011599365506429218, 40: 0.4750028149551481, 41: 0.0010720054935953304, 42: 0.006574459498380249, 43: 0.004373176349838724, 44: 0.006600522652480711, 45: 0.05462403066190879, 46: 0.002475315778340539, 47: 0.0016496344994175534, 48: 0.0162560565697898, 49: 0.0007953133076056048, 50: 0.08643813266238533, 51: 0.0003386656637208994, 52: 0.0043745867123679135, 53: 0.0007236105413913981, 54: 0.0012943107971993806, 55: 0.020353896257732103, 56: 0.002779384110351319, 57: 0.00026824048193324943, 58: 0.0007148367154989401, 59: 0.00014840543761513242, 60: 0.043008709545530134, 61: 7.752902617788323e-05, 62: 0.0003895966451490943, 63: 0.0003774625617129223, 64: 0.00042595603943586594, 65: 0.007006211463920572, 66: 0.0004102484978440283, 67: 9.925357481254628e-05, 68: 0.00022206727456640274, 69: 2.1828180324523158e-05, 70: 0.008170033015486801, 72: 0.002030608340272225, 73: 6.930606748683779e-05, 74: 5.515926232416409e-05, 75: 0.002009212606353514, 76: 8.792296082182094e-05, 77: 0.00022777176729449377, 78: 0.00029280540246749486, 79: 4.941504411496002e-06, 80: 0.004183963127649575, 81: 6.459751847817315e-05, 82: 3.0540043102419042e-06, 84: 0.0011761069804494678, 85: 0.0002735752470178291, 86: 5.313391048546066e-05, 87: 9.336783457687702e-06, 88: 0.00012357172238531235, 89: 3.8341350355346186e-05, 90: 0.000844305944634708, 91: 4.110939597138037e-05, 92: 7.604730891223761e-05, 94: 3.0337097709776337e-05, 95: 3.228450988173081e-05, 96: 0.0002801515413368683, 97: 3.592813532740593e-05, 98: 0.0002679081048779585, 99: 0.0024500539346919837}\n",
      "{40: 0.4750028149551481, 50: 0.08643813266238533, 45: 0.05462403066190879, 60: 0.043008709545530134, 35: 0.039750254268501276, 20: 0.03775265175785208, 30: 0.035563565627408374, 55: 0.020353896257732103, 25: 0.020016737139702544, 48: 0.0162560565697898, 38: 0.014070435839875153, 15: 0.012683411491090814, 32: 0.008798474473991942, 10: 0.00822093884464027, 70: 0.008170033015486801, 24: 0.007465276748769972, 36: 0.007376472162936601, 65: 0.007006211463920572, 44: 0.006600522652480711, 42: 0.006574459498380249, 16: 0.005952873923857416, 37: 0.004874414119077986, 12: 0.00464480234954009, 52: 0.0043745867123679135, 43: 0.004373176349838724, 8: 0.004306831773788993, 80: 0.004183963127649575, 28: 0.0029465861775135357, 56: 0.002779384110351319, 18: 0.002499484415610746, 46: 0.002475315778340539, 99: 0.0024500539346919837, 72: 0.002030608340272225, 75: 0.002009212606353514, 5: 0.001782997041601753, 6: 0.0017477536310751623, 47: 0.0016496344994175534, 4: 0.001502640719727734, 33: 0.0013127072572940571, 54: 0.0012943107971993806, 22: 0.0012101099412383685, 3: 0.0011775734768106653, 84: 0.0011761069804494678, 39: 0.0011599365506429218, 2: 0.001110879143807656, 41: 0.0010720054935953304, 14: 0.001068289757765461, 21: 0.0009668757860683624, 34: 0.0009296414596500113, 17: 0.0009196336610004795, 7: 0.0008876964252294569, 23: 0.000869856013920686, 90: 0.000844305944634708, 27: 0.0008371259957222337, 26: 0.0008324194977574308, 49: 0.0007953133076056048, 53: 0.0007236105413913981, 58: 0.0007148367154989401, 13: 0.0005934932903449739, 9: 0.0005425860578457053, 1: 0.0005355909187801483, 64: 0.00042595603943586594, 66: 0.0004102484978440283, 62: 0.0003895966451490943, 63: 0.0003774625617129223, 19: 0.00037384246929681955, 29: 0.0003484678182358651, 51: 0.0003386656637208994, 78: 0.00029280540246749486, 11: 0.00029105478255659766, 96: 0.0002801515413368683, 85: 0.0002735752470178291, 57: 0.00026824048193324943, 98: 0.0002679081048779585, 31: 0.00024778768844258833, 77: 0.00022777176729449377, 68: 0.00022206727456640274, 59: 0.00014840543761513242, 88: 0.00012357172238531235, 67: 9.925357481254628e-05, 76: 8.792296082182094e-05, 61: 7.752902617788323e-05, 92: 7.604730891223761e-05, 73: 6.930606748683779e-05, 81: 6.459751847817315e-05, 74: 5.515926232416409e-05, 86: 5.313391048546066e-05, 91: 4.110939597138037e-05, 89: 3.8341350355346186e-05, 97: 3.592813532740593e-05, 95: 3.228450988173081e-05, 94: 3.0337097709776337e-05, 69: 2.1828180324523158e-05, 87: 9.336783457687702e-06, 79: 4.941504411496002e-06, 82: 3.0540043102419042e-06}\n",
      "{'<40': 0.2381721704989729, '40': 0.4750028149551481, '>40': 0.28682501454587894}\n",
      "Average Hours: 40.32911200806684\n"
     ]
    }
   ],
   "source": [
    "hours_per_week_distribution = (df.groupby('hours-per-week')['fnlwgt'].sum() / total_fnlwgt).to_dict()\n",
    "print(hours_per_week_distribution)\n",
    "sorted_hours_per_week = dict(sorted(hours_per_week_distribution.items(), key=lambda item: item[1], reverse=True))\n",
    "print(sorted_hours_per_week)\n",
    "\n",
    "hours_per_week_distribution_grouped = {\"<40\":0, \"40\":0, \">40\":0}\n",
    "for key, value in hours_per_week_distribution.items():\n",
    "    if key < 40:\n",
    "        hours_per_week_distribution_grouped[\"<40\"] += value\n",
    "    elif key == 40:\n",
    "        hours_per_week_distribution_grouped[\"40\"] += value\n",
    "    else:\n",
    "        hours_per_week_distribution_grouped[\">40\"] += value\n",
    "print(hours_per_week_distribution_grouped)\n",
    "\n",
    "\n",
    "average_hours_per_week = 0\n",
    "for key, value in hours_per_week_distribution.items():\n",
    "    average_hours_per_week += key * value\n",
    "print(\"Average Hours:\" , average_hours_per_week)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd6877f",
   "metadata": {},
   "source": [
    "# Class Balance of the Dataset\n",
    "\n",
    "Check the distribution of the target variable (the class you’re trying to predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d395c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "income\n",
       "<=50K    0.760718\n",
       ">50K     0.239282\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['income'].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca8522a",
   "metadata": {},
   "source": [
    "## **Preprocessing the Dataset**\n",
    "\n",
    "In this section, we handle the preprocessing of the Adult Income dataset to prepare it for machine learning.\n",
    "\n",
    "### **Handling Missing Values**\n",
    "\n",
    "The dataset contains some missing values, represented as `\"NaN\"`, particularly in categorical features like `workclass`, `occupation`, and `native-country`. We will address them using two strategies:\n",
    "\n",
    "- **Removing rows with missing values** to eliminate uncertainty.\n",
    "- **Replacing missing values** with the most frequent value (mode) in the respective column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85fced8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: workclass\n",
      "  Most common value: Private\n",
      "  Frequency: 33906 (73.64% of non-missing values)\n",
      "\n",
      "Column: occupation\n",
      "  Most common value: Prof-specialty\n",
      "  Frequency: 6172 (13.41% of non-missing values)\n",
      "\n",
      "Column: native-country\n",
      "  Most common value: United-States\n",
      "  Frequency: 43832 (91.35% of non-missing values)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = ['workclass', 'occupation', 'native-country']\n",
    "\n",
    "for col in columns:\n",
    "    mode_value = df[col].mode()[0]\n",
    "    mode_count = df[col].value_counts().loc[mode_value]\n",
    "    total_non_missing = df[col].notna().sum()\n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\"  Most common value: {mode_value}\")\n",
    "    print(f\"  Frequency: {mode_count} ({mode_count / total_non_missing:.2%} of non-missing values)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718ddad8",
   "metadata": {},
   "source": [
    "### **Missing Values Chosen Strategy** \n",
    "\n",
    "Missing values in the workclass, occupation, and native-country columns were imputed using the most frequent value (mode) in each column. This strategy retains more data while introducing minimal bias, as the mode values were dominant — especially in native-country (>91%) and workclass (>73%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7874876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of rows: 48842\n",
      "Remaining rows after dropping: 45222\n",
      "Rows dropped: 3620\n",
      "Percentage of data lost: 7.41%\n"
     ]
    }
   ],
   "source": [
    "original_rows = df.shape[0]\n",
    "\n",
    "# Drop rows with missing values in the selected columns\n",
    "df_dropped = df.dropna(subset=['workclass', 'occupation', 'native-country'])\n",
    "\n",
    "# Number of rows after dropping\n",
    "remaining_rows = df_dropped.shape[0]\n",
    "\n",
    "# Number of rows removed\n",
    "rows_dropped = original_rows - remaining_rows\n",
    "\n",
    "# Percentage of data lost\n",
    "percent_lost = (rows_dropped / original_rows) * 100\n",
    "\n",
    "print(f\"Original number of rows: {original_rows}\")\n",
    "print(f\"Remaining rows after dropping: {remaining_rows}\")\n",
    "print(f\"Rows dropped: {rows_dropped}\")\n",
    "print(f\"Percentage of data lost: {percent_lost:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a569cd",
   "metadata": {},
   "source": [
    "### **Dropping Rows with missing Values**\n",
    "\n",
    "The dataset contains missing values in the workclass, occupation, and native-country columns. Dropping rows with missing values would result in a loss of 7.41% of the data (3,620 rows). To preserve the dataset size and avoid potential bias introduced by data loss, we chose to impute missing values using the most frequent (mode) value for each column. The dominant presence of these mode values (e.g., native-country = \"United-States\" at 91%) supports this choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ed2da4",
   "metadata": {},
   "source": [
    "### **Dataset After This Step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c93e305d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after imputation:\n",
      "age               0\n",
      "workclass         0\n",
      "fnlwgt            0\n",
      "education         0\n",
      "education-num     0\n",
      "marital-status    0\n",
      "occupation        0\n",
      "relationship      0\n",
      "race              0\n",
      "sex               0\n",
      "capital-gain      0\n",
      "capital-loss      0\n",
      "hours-per-week    0\n",
      "native-country    0\n",
      "income            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "columns_to_impute = ['workclass', 'occupation', 'native-country']\n",
    "for col in columns_to_impute:\n",
    "    mode_val = df[col].mode()[0]\n",
    "    df[col] = df[col].fillna(mode_val)\n",
    "\n",
    "# Check if there are any missing values left\n",
    "missing_values_after = df.isna().sum()\n",
    "print(\"Missing values after imputation:\")\n",
    "print(missing_values_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ae8793",
   "metadata": {},
   "source": [
    "### **Encoding Categorical Features**\n",
    "\n",
    "Machine learning models typically require numerical input. Therefore, we will encode categorical features such as `education`, `marital-status`, and `occupation` using:\n",
    "\n",
    "- **One-hot encoding** for features with no ordinal relationship.\n",
    "- **Label encoding** for features with inherent order (if applicable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "173151e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48842, 15)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "print(df.shape)\n",
    "# Label encode 'education' (ordinal)\n",
    "education_order = [\n",
    "    'Preschool', '1st-4th', '5th-6th', '7th-8th', '9th',\n",
    "    '10th', '11th', '12th', 'HS-grad',\n",
    "    'Some-college', 'Assoc-voc', 'Assoc-acdm',\n",
    "    'Bachelors', 'Masters', 'Doctorate'\n",
    "]\n",
    "\n",
    "df['education'] = pd.Categorical(df['education'], categories=education_order, ordered=True)\n",
    "df['education'] = df['education'].cat.codes\n",
    "\n",
    "# One-hot encode nominal categorical features\n",
    "one_hot_cols = [\n",
    "    'workclass', 'marital-status', 'occupation', 'relationship',\n",
    "    'race', 'sex', 'native-country'\n",
    "]\n",
    "\n",
    "df = pd.get_dummies(df, columns=one_hot_cols, drop_first=True)\n",
    "\n",
    "df['income'] = df['income'].map({'<=50K': 0, '>50K': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30080a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>income</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>workclass_Never-worked</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_Portugal</th>\n",
       "      <th>native-country_Puerto-Rico</th>\n",
       "      <th>native-country_Scotland</th>\n",
       "      <th>native-country_South</th>\n",
       "      <th>native-country_Taiwan</th>\n",
       "      <th>native-country_Thailand</th>\n",
       "      <th>native-country_Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_United-States</th>\n",
       "      <th>native-country_Vietnam</th>\n",
       "      <th>native-country_Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>77516</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>83311</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>215646</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>234721</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>338409</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  fnlwgt  education  education-num  capital-gain  capital-loss  \\\n",
       "0   39   77516         12             13          2174             0   \n",
       "1   50   83311         12             13             0             0   \n",
       "2   38  215646          8              9             0             0   \n",
       "3   53  234721          6              7             0             0   \n",
       "4   28  338409         12             13             0             0   \n",
       "\n",
       "   hours-per-week  income  workclass_Local-gov  workclass_Never-worked  ...  \\\n",
       "0              40       0                False                   False  ...   \n",
       "1              13       0                False                   False  ...   \n",
       "2              40       0                False                   False  ...   \n",
       "3              40       0                False                   False  ...   \n",
       "4              40       0                False                   False  ...   \n",
       "\n",
       "   native-country_Portugal  native-country_Puerto-Rico  \\\n",
       "0                    False                       False   \n",
       "1                    False                       False   \n",
       "2                    False                       False   \n",
       "3                    False                       False   \n",
       "4                    False                       False   \n",
       "\n",
       "   native-country_Scotland  native-country_South  native-country_Taiwan  \\\n",
       "0                    False                 False                  False   \n",
       "1                    False                 False                  False   \n",
       "2                    False                 False                  False   \n",
       "3                    False                 False                  False   \n",
       "4                    False                 False                  False   \n",
       "\n",
       "   native-country_Thailand  native-country_Trinadad&Tobago  \\\n",
       "0                    False                           False   \n",
       "1                    False                           False   \n",
       "2                    False                           False   \n",
       "3                    False                           False   \n",
       "4                    False                           False   \n",
       "\n",
       "   native-country_United-States  native-country_Vietnam  \\\n",
       "0                          True                   False   \n",
       "1                          True                   False   \n",
       "2                          True                   False   \n",
       "3                          True                   False   \n",
       "4                         False                   False   \n",
       "\n",
       "   native-country_Yugoslavia  \n",
       "0                      False  \n",
       "1                      False  \n",
       "2                      False  \n",
       "3                      False  \n",
       "4                      False  \n",
       "\n",
       "[5 rows x 84 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 5 rows to check encoding results\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775ff236",
   "metadata": {},
   "source": [
    "### **Enconding Resoning and Results**\n",
    "\n",
    "We encoded the categorical features to convert them into a numeric format suitable for machine learning algorithms. Since the education feature has a clear order of attainment levels, we applied label encoding based on its natural hierarchy, preserving the ordinal relationship. For other categorical features without inherent order—such as workclass, marital-status, occupation, relationship, race, sex, and native-country—we applied one-hot encoding, which created new binary columns for each category. This expanded the feature space but allowed models to interpret categorical data effectively. Lastly, the target variable income was binarized to 0 and 1, facilitating binary classification. This preprocessing step transformed all relevant columns into numeric form, enabling downstream model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55bff2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48842, 84)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f24e945",
   "metadata": {},
   "source": [
    "### **Why Does The Data Change Shape**\n",
    "\n",
    "The dataset’s number of columns increases after encoding because one-hot encoding creates a new binary column for each category in the original categorical features. This expands the feature space to convert categorical data into a numeric format suitable for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1458a990",
   "metadata": {},
   "source": [
    "### **Transforming Numerical Features**\n",
    "\n",
    "Some continuous numerical features contain extreme or highly granular values that could introduce noise:\n",
    "\n",
    "- `hours-per-week` includes values as low as 1 and as high as 99 — we may **clip or bin these values** into broader categories (e.g., part-time, full-time, overtime).\n",
    "- `capital-gain` and `capital-loss` have **many rare and highly specific values**, most of which are zero — we may **bucketize or simplify** these into categories like \"none\", \"low\", and \"high\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fa0ba95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top capital-gain values:\n",
      "capital-gain\n",
      "0        44807\n",
      "15024      513\n",
      "7688       410\n",
      "7298       364\n",
      "99999      244\n",
      "3103       152\n",
      "5178       146\n",
      "5013       117\n",
      "4386       108\n",
      "8614        82\n",
      "3325        81\n",
      "2174        74\n",
      "10520       64\n",
      "4650        63\n",
      "27828       58\n",
      "4064        54\n",
      "594         52\n",
      "3137        51\n",
      "14084       49\n",
      "20051       49\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top capital-loss values:\n",
      "capital-loss\n",
      "0       46560\n",
      "1902      304\n",
      "1977      253\n",
      "1887      233\n",
      "2415       72\n",
      "1485       71\n",
      "1848       67\n",
      "1590       62\n",
      "1602       62\n",
      "1876       59\n",
      "1740       58\n",
      "1672       50\n",
      "1741       44\n",
      "1564       43\n",
      "2258       39\n",
      "1719       38\n",
      "1980       36\n",
      "1408       35\n",
      "1669       35\n",
      "2001       35\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot capital-gain distribution (log scale to handle skewness)\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df['capital-gain'], bins=50, color='skyblue')\n",
    "plt.title('Capital Gain Distribution')\n",
    "plt.xlabel('Capital Gain')\n",
    "plt.ylabel('Frequency')\n",
    "plt.yscale('log')  # Log scale for better visibility of rare high values\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df['capital-loss'], bins=50, color='salmon')\n",
    "plt.title('Capital Loss Distribution')\n",
    "plt.xlabel('Capital Loss')\n",
    "plt.ylabel('Frequency')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Also, print value counts for the top values to see where to place threshold\n",
    "print(\"Top capital-gain values:\")\n",
    "print(df['capital-gain'].value_counts().head(20))\n",
    "\n",
    "print(\"\\nTop capital-loss values:\")\n",
    "print(df['capital-loss'].value_counts().head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7394c5f3",
   "metadata": {},
   "source": [
    "### **Binning Strategy**\n",
    "\n",
    "To simplify the numerical features and reduce noise from extreme values, we binned capital-gain and capital-loss into three categories: \"none\" for zero values, \"low\" for values up to a chosen threshold, and \"high\" for values above it. Based on data distribution analysis, we selected 7,000 as the threshold for capital-gain and 2,000 for capital-loss. Additionally, we transformed hours-per-week into categorical bins representing work intensity: \"part-time\" (≤30 hours), \"full-time\" (31–40 hours), and \"overtime\" (>40 hours). This binning strategy groups continuous values into meaningful categories, helping reduce noise and improving model interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ebd5ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48842, 90)\n",
      "   age  fnlwgt  education  education-num  income  workclass_Local-gov  \\\n",
      "0   39   77516         12             13       0                False   \n",
      "1   50   83311         12             13       0                False   \n",
      "2   38  215646          8              9       0                False   \n",
      "3   53  234721          6              7       0                False   \n",
      "4   28  338409         12             13       0                False   \n",
      "\n",
      "   workclass_Never-worked  workclass_Private  workclass_Self-emp-inc  \\\n",
      "0                   False              False                   False   \n",
      "1                   False              False                   False   \n",
      "2                   False               True                   False   \n",
      "3                   False               True                   False   \n",
      "4                   False               True                   False   \n",
      "\n",
      "   workclass_Self-emp-not-inc  ...  native-country_Yugoslavia  \\\n",
      "0                       False  ...                      False   \n",
      "1                        True  ...                      False   \n",
      "2                       False  ...                      False   \n",
      "3                       False  ...                      False   \n",
      "4                       False  ...                      False   \n",
      "\n",
      "   hours-per-week-binned_full-time  hours-per-week-binned_overtime  \\\n",
      "0                             True                           False   \n",
      "1                            False                           False   \n",
      "2                             True                           False   \n",
      "3                             True                           False   \n",
      "4                             True                           False   \n",
      "\n",
      "   hours-per-week-binned_part-time  capital-gain-binned_high  \\\n",
      "0                            False                     False   \n",
      "1                             True                     False   \n",
      "2                            False                     False   \n",
      "3                            False                     False   \n",
      "4                            False                     False   \n",
      "\n",
      "   capital-gain-binned_low  capital-gain-binned_none  \\\n",
      "0                     True                     False   \n",
      "1                    False                      True   \n",
      "2                    False                      True   \n",
      "3                    False                      True   \n",
      "4                    False                      True   \n",
      "\n",
      "   capital-loss-binned_high  capital-loss-binned_low  capital-loss-binned_none  \n",
      "0                     False                    False                      True  \n",
      "1                     False                    False                      True  \n",
      "2                     False                    False                      True  \n",
      "3                     False                    False                      True  \n",
      "4                     False                    False                      True  \n",
      "\n",
      "[5 rows x 90 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Copy original dataframe\n",
    "df_transformed = df.copy()\n",
    "\n",
    "# Binning hours-per-week\n",
    "def hours_bin(hours):\n",
    "    if hours <= 30:\n",
    "        return 'part-time'\n",
    "    elif hours <= 40:\n",
    "        return 'full-time'\n",
    "    else:\n",
    "        return 'overtime'\n",
    "\n",
    "df_transformed['hours-per-week-binned'] = df_transformed['hours-per-week'].apply(hours_bin)\n",
    "\n",
    "# Binning capital-gain\n",
    "def capital_gain_bin(gain):\n",
    "    if gain == 0:\n",
    "        return 'none'\n",
    "    elif gain <= 7000:\n",
    "        return 'low'\n",
    "    else:\n",
    "        return 'high'\n",
    "\n",
    "df_transformed['capital-gain-binned'] = df_transformed['capital-gain'].apply(capital_gain_bin)\n",
    "\n",
    "# Binning capital-loss\n",
    "def capital_loss_bin(loss):\n",
    "    if loss == 0:\n",
    "        return 'none'\n",
    "    elif loss <= 2000:\n",
    "        return 'low'\n",
    "    else:\n",
    "        return 'high'\n",
    "\n",
    "df_transformed['capital-loss-binned'] = df_transformed['capital-loss'].apply(capital_loss_bin)\n",
    "\n",
    "# Drop original continuous columns\n",
    "df_transformed = df_transformed.drop(columns=['hours-per-week', 'capital-gain', 'capital-loss'])\n",
    "\n",
    "# One-hot encode the new binned columns\n",
    "binned_cols = ['hours-per-week-binned', 'capital-gain-binned', 'capital-loss-binned']\n",
    "df_transformed = pd.get_dummies(df_transformed, columns=binned_cols)\n",
    "# Check results\n",
    "print(df_transformed.shape)\n",
    "print(df_transformed.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18171560",
   "metadata": {},
   "source": [
    "### **2 Versions Of Our Dataset**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1beb2f",
   "metadata": {},
   "source": [
    "This step of transforming numerical features into binned categories was considered optional. To evaluate its impact on model performance, we decided to create and retain two versions of the dataset: one with the original continuous numerical features, and another with the binned and encoded versions. This approach allows for a direct comparison to determine which preprocessing strategy yields better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61f7232d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()\n",
    "df2 = df_transformed.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5036be56",
   "metadata": {},
   "source": [
    "### **Splitting the Data**\n",
    "\n",
    "We will explore one approaches to splitting the dataset for training and testing:\n",
    "\n",
    "- Perform **random train/test splits** (e.g., 80/20 or 70/30) using `train_test_split` for experimentation and validation.\n",
    "\n",
    "### **Optional: Addressing Class Imbalance**\n",
    "\n",
    "If we find that the dataset is imbalanced (e.g., far more samples with income ≤50K than >50K), we may apply techniques to balance the classes:\n",
    "\n",
    "- **Oversampling** the minority class.\n",
    "- **Undersampling** the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9224326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df1 and df2 include features + target 'income'\n",
    "X1, y1 = df1.drop(columns='income'), df1['income']\n",
    "X2, y2 = df2.drop(columns='income'), df2['income']\n",
    "\n",
    "def generate_splits(X, y, n_splits=10, test_size=0.2, random_state=42):\n",
    "    raw_splits = []\n",
    "    over_splits = []\n",
    "    under_splits = []\n",
    "    \n",
    "    ros = RandomOverSampler(random_state=random_state)\n",
    "    rus = RandomUnderSampler(random_state=random_state)\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        rs = random_state + i  # change seed for diversity\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, stratify=y, random_state=rs\n",
    "        )\n",
    "        \n",
    "        # Raw (no sampling)\n",
    "        raw_splits.append((X_train, X_test, y_train, y_test))\n",
    "        \n",
    "        # Oversampling only\n",
    "        X_over, y_over = ros.fit_resample(X_train, y_train)\n",
    "        over_splits.append((X_over, X_test, y_over, y_test))\n",
    "        \n",
    "        # Undersampling only\n",
    "        X_under, y_under = rus.fit_resample(X_train, y_train)\n",
    "        under_splits.append((X_under, X_test, y_under, y_test))\n",
    "        \n",
    "    \n",
    "    return raw_splits, over_splits, under_splits\n",
    "\n",
    "# Generate splits for df1\n",
    "df1_raw, df1_OverSampling, df1_UnderSampling = generate_splits(X1, y1)\n",
    "\n",
    "# Generate splits for df2\n",
    "df2_raw, df2_OverSampling, df2_UnderSampling = generate_splits(X2, y2)\n",
    "\n",
    "# Example of accessing a split:\n",
    "# X_train, X_test, y_train, y_test = df1_OverSampling[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee69b64",
   "metadata": {},
   "source": [
    "### **Splitting and sampling Strategy**\n",
    "\n",
    "\n",
    "We created multiple random 80/20 train-test splits for each dataset version to ensure robust evaluation across different data partitions. To address the class imbalance present in the income target variable, we generated splits under four conditions:\n",
    "\n",
    "Raw data with no sampling applied,\n",
    "\n",
    "Oversampling of the minority class,\n",
    "\n",
    "Undersampling of the majority class, and\n",
    "\n",
    "Stratified splitting was used to maintain the original class distribution in each split, ensuring consistent representation of both income classes during training and testing. This approach allows us to compare the impact of different sampling techniques on model performance more reliably."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc1a763",
   "metadata": {},
   "source": [
    "## **Model Implementation**\n",
    "\n",
    "In this section, we will implement and evaluate various machine learning models to predict whether an individual's income exceeds $50K based on the features in the dataset. The objective is to confirm and apply machine learning theory learned in class, deepen our understanding of classification models, and identify the most effective approach for this problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f160bd",
   "metadata": {},
   "source": [
    "### **Evaluation Metrics**\n",
    "\n",
    "All models will be evaluated using key classification metrics to gain a comprehensive view of performance:\n",
    "\n",
    "- **Accuracy**  \n",
    "  The proportion of total predictions that are correct. Good for balanced datasets.\n",
    "\n",
    "- **Precision**  \n",
    "  The proportion of positive predictions that were actually correct. Important when false positives are costly.\n",
    "\n",
    "- **Recall**  \n",
    "  The proportion of actual positives that were correctly predicted. Important when false negatives are costly.\n",
    "\n",
    "- **F1-Score**  \n",
    "  The harmonic mean of precision and recall. Useful when you need a balance between precision and recall.\n",
    "\n",
    "- **ROC AUC Score**  \n",
    "  Measures the ability of the model to distinguish between classes across different thresholds. A higher score indicates better overall classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5508e062",
   "metadata": {},
   "source": [
    "\n",
    "### **Implemented Models**\n",
    "\n",
    "We will start by implementing the following classification models:\n",
    "\n",
    "- **Logistic Regression**  \n",
    "  A simple and interpretable linear model; it's a good baseline for binary classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73e7e832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Average Performance by Dataset:\n",
      "                   accuracy  precision  recall      f1  roc_auc\n",
      "dataset                                                        \n",
      "df1_OverSampling     0.8055     0.5624  0.8454  0.6754   0.9034\n",
      "df1_UnderSampling    0.8045     0.5608  0.8458  0.6744   0.9032\n",
      "df1_raw              0.8504     0.7299  0.5951  0.6556   0.9036\n",
      "df2_OverSampling     0.8106     0.5704  0.8457  0.6813   0.9077\n",
      "df2_UnderSampling    0.8101     0.5696  0.8463  0.6809   0.9076\n",
      "df2_raw              0.8547     0.7480  0.5929  0.6614   0.9079\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# List of dataset split arrays (you must already have these defined)\n",
    "datasets = {\n",
    "    'df1_raw': df1_raw,\n",
    "    'df1_OverSampling': df1_OverSampling,\n",
    "    'df1_UnderSampling': df1_UnderSampling,\n",
    "    'df2_raw': df2_raw,\n",
    "    'df2_OverSampling': df2_OverSampling,\n",
    "    'df2_UnderSampling': df2_UnderSampling,\n",
    "}\n",
    "\n",
    "# Initialize empty list to collect all results\n",
    "results = []\n",
    "\n",
    "for name, splits in datasets.items():\n",
    "    \n",
    "    for i, (X_train, X_test, y_train, y_test) in enumerate(splits):\n",
    "        \n",
    "        # Create pipeline with scaling\n",
    "        model = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
    "\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Compute metrics\n",
    "        metrics = {\n",
    "            'dataset': name,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "        }\n",
    "\n",
    "        results.append(metrics)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Show average metrics per dataset\n",
    "print(\"\\n Average Performance by Dataset:\")\n",
    "print(results_df.groupby('dataset').mean(numeric_only=True).round(4))\n",
    "\n",
    "# Optional: save to CSV\n",
    "results_df.to_csv(\"logistic_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222fe74",
   "metadata": {},
   "source": [
    "### **Conclusoes**\n",
    "\n",
    "falar do que se fez \n",
    "falar da pipeline e dizer que diminui muito o tempo de treino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d9b1fe",
   "metadata": {},
   "source": [
    "- **Random Forest Classifier**  \n",
    "  A robust ensemble method that reduces overfitting; chosen for its ability to handle non-linear relationships and mixed feature types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f421044",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m model \u001b[38;5;241m=\u001b[39m make_pipeline(StandardScaler(), RandomForestClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_estimators\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m))\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[1;32m     29\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/anaconda3/envs/taa/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/taa/lib/python3.10/site-packages/sklearn/pipeline.py:662\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    657\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[1;32m    658\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    659\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[1;32m    660\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    661\u001b[0m         )\n\u001b[0;32m--> 662\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/taa/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/taa/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:487\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    476\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    479\u001b[0m ]\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 487\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/anaconda3/envs/taa/lib/python3.10/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/taa/lib/python3.10/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/anaconda3/envs/taa/lib/python3.10/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/anaconda3/envs/taa/lib/python3.10/site-packages/sklearn/utils/parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/taa/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:189\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    187\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 189\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m    198\u001b[0m         X,\n\u001b[1;32m    199\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    203\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/taa/lib/python3.10/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# List of dataset split arrays (already defined)\n",
    "datasets = {\n",
    "    'df1_raw': df1_raw,\n",
    "    'df1_OverSampling': df1_OverSampling,\n",
    "    'df1_UnderSampling': df1_UnderSampling,\n",
    "    'df2_raw': df2_raw,\n",
    "    'df2_OverSampling': df2_OverSampling,\n",
    "    'df2_UnderSampling': df2_UnderSampling,\n",
    "}\n",
    "\n",
    "# Initialize empty list to collect all results\n",
    "rf_results = []\n",
    "\n",
    "for name, splits in datasets.items():\n",
    "    \n",
    "    for i, (X_train, X_test, y_train, y_test) in enumerate(splits):\n",
    "\n",
    "        # Initialize Random Forest model\n",
    "        model = make_pipeline(StandardScaler(), RandomForestClassifier(random_state=42, n_estimators= 100))\n",
    "\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Compute metrics\n",
    "        metrics = {\n",
    "            'dataset': name,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "        }\n",
    "\n",
    "        rf_results.append(metrics)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "rf_results_df = pd.DataFrame(rf_results)\n",
    "\n",
    "# Show average metrics per dataset\n",
    "print(\"\\n📊 Average Random Forest Performance by Dataset:\")\n",
    "print(rf_results_df.groupby('dataset').mean(numeric_only=True).round(4))\n",
    "\n",
    "# Optional: save to CSV\n",
    "rf_results_df.to_csv(\"random_forest_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fd0a8e",
   "metadata": {},
   "source": [
    "### **Conclusoes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ca133c",
   "metadata": {},
   "source": [
    "- **Gradient Boosting (XGBoost or LightGBM)**  \n",
    "  A highly accurate boosting algorithm; selected for its strong performance in structured/tabular data and ability to capture complex patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cb3580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Average XGBoost Performance by Dataset:\n",
      "                   accuracy  precision  recall      f1  roc_auc\n",
      "dataset                                                        \n",
      "df1_OverSampling     0.8357     0.6135  0.8472  0.7116   0.9268\n",
      "df1_UnderSampling    0.8264     0.5943  0.8659  0.7049   0.9251\n",
      "df1_raw              0.8711     0.7740  0.6517  0.7075   0.9275\n",
      "df2_OverSampling     0.8194     0.5868  0.8291  0.6872   0.9101\n",
      "df2_UnderSampling    0.8092     0.5675  0.8523  0.6813   0.9101\n",
      "df2_raw              0.8582     0.7447  0.6202  0.6767   0.9122\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# List of dataset split arrays (already defined)\n",
    "datasets = {\n",
    "    'df1_raw': df1_raw,\n",
    "    'df1_OverSampling': df1_OverSampling,\n",
    "    'df1_UnderSampling': df1_UnderSampling,\n",
    "    'df2_raw': df2_raw,\n",
    "    'df2_OverSampling': df2_OverSampling,\n",
    "    'df2_UnderSampling': df2_UnderSampling,\n",
    "}\n",
    "\n",
    "# Initialize list to collect results\n",
    "xgb_results = []\n",
    "\n",
    "for name, splits in datasets.items():\n",
    "    \n",
    "    for i, (X_train, X_test, y_train, y_test) in enumerate(splits):\n",
    "\n",
    "        # Initialize XGBoost model\n",
    "        model =  XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "\n",
    "        # Train\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Metrics\n",
    "        metrics = {\n",
    "            'dataset': name,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "        }\n",
    "\n",
    "        xgb_results.append(metrics)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "xgb_results_df = pd.DataFrame(xgb_results)\n",
    "\n",
    "# Show average metrics per dataset\n",
    "print(\"📊 Average XGBoost Performance by Dataset:\")\n",
    "print(xgb_results_df.groupby('dataset').mean(numeric_only=True).round(4))\n",
    "\n",
    "# Optional: Save to CSV\n",
    "xgb_results_df.to_csv(\"xgboost_results.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "85d56a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Average XGBoost Performance by Dataset:\n",
      "                   accuracy  precision  recall      f1  roc_auc\n",
      "dataset                                                        \n",
      "df1_OverSampling     0.8357     0.9454  0.8321  0.8851   0.9268\n",
      "df1_UnderSampling    0.8264     0.9507  0.8140  0.8771   0.9251\n",
      "df1_raw              0.8711     0.8956  0.9401  0.9173   0.9275\n",
      "df2_OverSampling     0.8194     0.9382  0.8163  0.8730   0.9101\n",
      "df2_UnderSampling    0.8092     0.9448  0.7956  0.8638   0.9101\n",
      "df2_raw              0.8582     0.8865  0.9331  0.9092   0.9122\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# List of dataset split arrays (already defined)\n",
    "datasets = {\n",
    "    'df1_raw': df1_raw,\n",
    "    'df1_OverSampling': df1_OverSampling,\n",
    "    'df1_UnderSampling': df1_UnderSampling,\n",
    "    'df2_raw': df2_raw,\n",
    "    'df2_OverSampling': df2_OverSampling,\n",
    "    'df2_UnderSampling': df2_UnderSampling,\n",
    "}\n",
    "\n",
    "# Initialize list to collect results\n",
    "xgb_results = []\n",
    "\n",
    "for name, splits in datasets.items():\n",
    "    \n",
    "    for i, (X_train, X_test, y_train, y_test) in enumerate(splits):\n",
    "\n",
    "        # Initialize XGBoost model\n",
    "        model =  XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "\n",
    "        # Train\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Metrics\n",
    "        metrics = {\n",
    "            'dataset': name,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred, pos_label=0),\n",
    "            'recall': recall_score(y_test, y_pred, pos_label=0),\n",
    "            'f1': f1_score(y_test, y_pred, pos_label=0),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "        }\n",
    "\n",
    "        xgb_results.append(metrics)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "xgb_results_df = pd.DataFrame(xgb_results)\n",
    "\n",
    "# Show average metrics per dataset\n",
    "print(\"📊 Average XGBoost Performance by Dataset:\")\n",
    "print(xgb_results_df.groupby('dataset').mean(numeric_only=True).round(4))\n",
    "\n",
    "# Optional: Save to CSV\n",
    "xgb_results_df.to_csv(\"xgboost_results.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9020f7",
   "metadata": {},
   "source": [
    "### **Conclusoes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a828035",
   "metadata": {},
   "source": [
    "### **Hyperparameter Tuning and Model Selection**\n",
    "\n",
    "To improve model performance, we may apply:\n",
    "\n",
    "- **Cross-validation**  \n",
    "  Using k-fold cross-validation to evaluate model reliability across different data splits.\n",
    "\n",
    "- **Grid Search or Randomized Search**  \n",
    "  For systematic tuning of hyperparameters, especially in Random Forest and Gradient Boosting models.\n",
    "\n",
    "As an enhancement, we performed hyperparameter tuning using GridSearchCV for Random Forest and RandomizedSearchCV for XGBoost. This process involved k-fold cross-validation to systematically evaluate model performance across different parameter settings. The objective was to identify configurations that improve classification metrics such as F1-score and ROC AUC, thereby strengthening the model's generalization capability. Once optimal parameters were found, the models were re-evaluated to confirm the improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fc50bf",
   "metadata": {},
   "source": [
    "### **Random Forest Hyperparameter Tuning**\n",
    "\n",
    "We will find 2 different tunes. One for each of these precision and recall\n",
    "\n",
    "We will use the best dataset accordingly.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd069b5f",
   "metadata": {},
   "source": [
    "📊 Average Random Forest Performance by Dataset:\n",
    "                   accuracy  precision  recall      f1  roc_auc\n",
    "dataset                                                        \n",
    "df1_OverSampling     0.8467     0.6790  0.6816  0.6803   0.8996\n",
    "df1_UnderSampling    0.8124     0.5748  0.8307  0.6794   0.9030\n",
    "df1_raw              0.8539     0.7298  0.6185  0.6695   0.9025\n",
    "df2_OverSampling     0.8345     0.6511  0.6645  0.6577   0.8886\n",
    "df2_UnderSampling    0.8059     0.5655  0.8159  0.6680   0.8952\n",
    "df2_raw              0.8426     0.6988  0.6018  0.6467   0.8921"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ae7886",
   "metadata": {},
   "source": [
    "maximize precision in df1_raw\n",
    "\n",
    "maximize recall in df1_UnderSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7149e17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Best parameters for Precision: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "✅ Best Precision from CV: 0.792846480758207\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "X_train, X_test, y_train, y_test = df1_raw[0]\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=cv, scoring='precision', n_jobs=-1, verbose=0)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n🎯 Best parameters for Precision:\", grid_search.best_params_)\n",
    "print(\"✅ Best Precision from CV:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7947c2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Average Random Forest Performance by Dataset:\n",
      "         accuracy  precision  recall      f1  roc_auc\n",
      "dataset                                              \n",
      "df1_raw    0.8572     0.7934  0.5453  0.6463    0.912\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# List of dataset split arrays (already defined)\n",
    "datasets = {\n",
    "    'df1_raw': df1_raw,\n",
    "}\n",
    "\n",
    "# Initialize empty list to collect all results\n",
    "rf_results = []\n",
    "\n",
    "for name, splits in datasets.items():\n",
    "    \n",
    "    for i, (X_train, X_test, y_train, y_test) in enumerate(splits):\n",
    "\n",
    "        # Initialize Random Forest model\n",
    "        model = make_pipeline(StandardScaler(),RandomForestClassifier(random_state=42, max_depth= 10, min_samples_leaf= 1, min_samples_split=2, n_estimators=200))\n",
    "\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Compute metrics\n",
    "        metrics = {\n",
    "            'dataset': name,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "        }\n",
    "\n",
    "        rf_results.append(metrics)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "rf_results_df = pd.DataFrame(rf_results)\n",
    "\n",
    "# Show average metrics per dataset\n",
    "print(\"\\n📊 Average Random Forest Performance by Dataset:\")\n",
    "print(rf_results_df.groupby('dataset').mean(numeric_only=True).round(4))\n",
    "\n",
    "# Optional: save to CSV\n",
    "#rf_results_df.to_csv(\"random_forest_results.csv\", index=False)\n",
    "#before hyperparameter tuning\n",
    "#df1_raw              0.8539     0.7298  0.6185  0.6695   0.9025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9134a3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Best parameters for Recall (Classe 0): {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "✅ Best Recall (Classe 0) from CV: 0.8067176533534761\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "\n",
    "# Split do conjunto\n",
    "X_train, X_test, y_train, y_test = df1_UnderSampling[0]\n",
    "\n",
    "# Hiperparâmetros para ajuste\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Modelo base\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Validação cruzada estratificada\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Scorer para recall da classe 0 (pobres)\n",
    "recall_class0 = make_scorer(recall_score, pos_label=0)\n",
    "\n",
    "# GridSearch com foco na classe 0\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=cv,\n",
    "                           scoring=recall_class0, n_jobs=-1, verbose=0)\n",
    "\n",
    "# Treinamento\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Resultados\n",
    "print(\"\\n📈 Best parameters for Recall (Classe 0):\", grid_search.best_params_)\n",
    "print(\"✅ Best Recall (Classe 0) from CV:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a1def55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Average Random Forest Performance by Dataset (Classe 0 como positiva):\n",
      "                   accuracy  precision  recall      f1  roc_auc\n",
      "dataset                                                        \n",
      "df1_UnderSampling    0.8154      0.942   0.807  0.8693   0.9087\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# Conjunto de dados\n",
    "datasets = {\n",
    "    'df1_UnderSampling': df1_UnderSampling,\n",
    "}\n",
    "\n",
    "# Lista de resultados\n",
    "rf_results = []\n",
    "\n",
    "for name, splits in datasets.items():\n",
    "    \n",
    "    for i, (X_train, X_test, y_train, y_test) in enumerate(splits):\n",
    "\n",
    "        # Modelo com os melhores parâmetros encontrados\n",
    "        model = make_pipeline(StandardScaler(), RandomForestClassifier(\n",
    "            random_state=42,\n",
    "            max_depth=None,\n",
    "            min_samples_leaf=1,\n",
    "            min_samples_split=5,\n",
    "            n_estimators=100\n",
    "        ))\n",
    "\n",
    "        # Treinamento\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predição\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Métricas (todas ajustadas para considerar classe 0 como positiva)\n",
    "        metrics = {\n",
    "            'dataset': name,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred, pos_label=0),\n",
    "            'recall': recall_score(y_test, y_pred, pos_label=0),\n",
    "            'f1': f1_score(y_test, y_pred, pos_label=0),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba)  # continua sendo com classe 1 como positiva\n",
    "        }\n",
    "\n",
    "        rf_results.append(metrics)\n",
    "\n",
    "# DataFrame com resultados\n",
    "rf_results_df = pd.DataFrame(rf_results)\n",
    "\n",
    "# Média por dataset\n",
    "print(\"\\n📊 Average Random Forest Performance by Dataset (Classe 0 como positiva):\")\n",
    "print(rf_results_df.groupby('dataset').mean(numeric_only=True).round(4))\n",
    "\n",
    "# Salvar (opcional)\n",
    "# rf_results_df.to_csv(\"rf_class0_recall_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79299a8",
   "metadata": {},
   "source": [
    "### **XGboost Hyperparameter Tuning**\n",
    "\n",
    "We will find 3 different tunes. One for each of these precision and recall\n",
    "\n",
    "We will use the best dataset accordingly.\n",
    "\n",
    "For example:\n",
    "\n",
    "📊 Average XGBoost Performance by Dataset:\n",
    "                   accuracy  precision  recall      f1  roc_auc\n",
    "dataset                                                        \n",
    "df1_OverSampling     0.8357     0.6135  0.8472  0.7116   0.9268\n",
    "df1_UnderSampling    0.8264     0.5943  0.8659  0.7049   0.9251\n",
    "df1_raw              0.8711     0.7740  0.6517  0.7075   0.9275\n",
    "df2_OverSampling     0.8194     0.5868  0.8291  0.6872   0.9101\n",
    "df2_UnderSampling    0.8092     0.5675  0.8523  0.6813   0.9101\n",
    "df2_raw              0.8582     0.7447  0.6202  0.6767   0.9122\n",
    "\n",
    "\n",
    "\n",
    "maximize precision df1_raw\n",
    "maximize recall in df1_UnderSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdf99646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "# Define parameter grid for XGBoost\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Common cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcf72e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Best parameters for Precision: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "✅ Best Precision from CV: 0.9863354706941946\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = df1_raw[0]\n",
    "\n",
    "grid_search_precision = GridSearchCV(\n",
    "    estimator=XGBClassifier(eval_metric='logloss', random_state=42),\n",
    "    param_grid=xgb_param_grid,\n",
    "    cv=cv,\n",
    "    scoring='precision',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "grid_search_precision.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n🎯 Best parameters for Precision:\", grid_search_precision.best_params_)\n",
    "print(\"✅ Best Precision from CV:\", grid_search_precision.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0dd2b7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Average XGBoost Performance by Dataset:\n",
      "         accuracy  precision  recall      f1  roc_auc\n",
      "dataset                                              \n",
      "df1_raw    0.8158     0.9846   0.234  0.3781   0.8955\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# List of dataset split arrays (already defined)\n",
    "datasets = {\n",
    "    'df1_raw': df1_raw\n",
    "}\n",
    "\n",
    "# Initialize list to collect results\n",
    "xgb_results = []\n",
    "\n",
    "for name, splits in datasets.items():\n",
    "    \n",
    "    for i, (X_train, X_test, y_train, y_test) in enumerate(splits):\n",
    "\n",
    "        # Initialize XGBoost model\n",
    "        model = XGBClassifier(eval_metric='logloss', random_state=42, colsample_bytree=0.8, learning_rate=0.01, max_depth= 3, n_estimators= 100, subsample= 1.0)\n",
    "\n",
    "        # Train\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Metrics\n",
    "        metrics = {\n",
    "            'dataset': name,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "        }\n",
    "\n",
    "        xgb_results.append(metrics)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "xgb_results_df = pd.DataFrame(xgb_results)\n",
    "\n",
    "# Show average metrics per dataset\n",
    "print(\"\\n📊 Average XGBoost Performance by Dataset:\")\n",
    "print(xgb_results_df.groupby('dataset').mean(numeric_only=True).round(4))\n",
    "\n",
    "# Optional: Save to CSV\n",
    "#xgb_results_df.to_csv(\"xgboost_results.csv\", index=False)\n",
    "\n",
    "#before hyperparameter tuning\n",
    "#df1_raw              0.8711     0.7740  0.6517  0.7075   0.9275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82d33e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Best parameters for Recall (classe 0): {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "✅ Best Recall (classe 0) from CV: 0.9989907202292889\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Split do dataset\n",
    "X_train, X_test, y_train, y_test = df1_raw[0]\n",
    "\n",
    "# Grid de parâmetros (exemplo, substitua pelo seu)\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Validação cruzada\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Scorer focado em recall da classe 0\n",
    "recall_class0 = make_scorer(recall_score, pos_label=0)\n",
    "\n",
    "# GridSearch\n",
    "grid_search_recall = GridSearchCV(\n",
    "    estimator=XGBClassifier(eval_metric='logloss', random_state=42),\n",
    "    param_grid=xgb_param_grid,\n",
    "    cv=cv,\n",
    "    scoring=recall_class0,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "grid_search_recall.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n📈 Best parameters for Recall (classe 0):\", grid_search_recall.best_params_)\n",
    "print(\"✅ Best Recall (classe 0) from CV:\", grid_search_recall.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "83e165ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Average XGBoost Performance by Dataset (classe 0 como positiva):\n",
      "                   accuracy  precision  recall      f1  roc_auc\n",
      "dataset                                                        \n",
      "df1_UnderSampling    0.8158     0.8056  0.9988  0.8919   0.8955\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# Dataset\n",
    "datasets = {\n",
    "    'df1_UnderSampling': df1_raw,\n",
    "}\n",
    "\n",
    "xgb_results = []\n",
    "\n",
    "for name, splits in datasets.items():\n",
    "    \n",
    "    for i, (X_train, X_test, y_train, y_test) in enumerate(splits):\n",
    "\n",
    "        # Modelo com melhores parâmetros encontrados\n",
    "        model = XGBClassifier(\n",
    "            eval_metric='logloss',\n",
    "            random_state=42,\n",
    "            colsample_bytree=0.8,\n",
    "            learning_rate=0.01,\n",
    "            max_depth=3,\n",
    "            n_estimators=100,\n",
    "            subsample=1\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Métricas com foco na classe 0\n",
    "        metrics = {\n",
    "            'dataset': name,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred, pos_label=0),\n",
    "            'recall': recall_score(y_test, y_pred, pos_label=0),\n",
    "            'f1': f1_score(y_test, y_pred, pos_label=0),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba)  # Classe 1 como positiva\n",
    "        }\n",
    "\n",
    "        xgb_results.append(metrics)\n",
    "\n",
    "# DataFrame\n",
    "xgb_results_df = pd.DataFrame(xgb_results)\n",
    "\n",
    "print(\"\\n📊 Average XGBoost Performance by Dataset (classe 0 como positiva):\")\n",
    "print(xgb_results_df.groupby('dataset').mean(numeric_only=True).round(4))\n",
    "\n",
    "# Exportar se quiser\n",
    "# xgb_results_df.to_csv(\"xgboost_results_class0.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c707aac",
   "metadata": {},
   "source": [
    "### Fairness analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2a5f9e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
